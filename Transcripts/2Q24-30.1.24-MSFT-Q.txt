"Operator

[Operator instructions] And our first question comes from the line of Mark Moerdler with Bernstein Research. Please proceed.

Mark Moerdler -- AllianceBernstein -- Analyst

Thank you very much. Congratulations on the strong quarter and thanks for letting me ask the question. Amy, you've discussed Azure being stable and you deliver Azure growth stability, but if we drill in one layer, we see Azure AI continuing to become a bigger portion of the revenue. I understand that separating what is directly AI revenue versus other IaaS, PaaS revenue that are leveraging well driven by AI is difficult, can you help me with two related questions? Optimization has been stabilizing and at some point, it should be part of the revenue flow.

How should we think about what happens then, do we see non-directly AI consumption being flattish or do we see a rebound as the cloud shift continues and the need for data of inferencing grows? Second point, on AI, where are we in the journey from training driving most of Azure AI usage to inferencing? When do you think we start to see pickup in non-Microsoft inferencing kick in, when do you think we could hit the point where inferencing is the bigger part of the driver? Thank you.

Satya Nadella -- Chief Executive Officer

You want me to go first and --

Amy Hood -- Chief Financial Officer

You go first, and I'll take the technical.

Satya Nadella -- Chief Executive Officer

Yes, let me -- just on the inferencing and training, most of what you've seen for the most part is all inferencing. So none of the large model training stuff is in any of our higher numbers at all. What small batch training, so somebody is doing fine-tuning or what have you, that will be there, but that's sort of a minor part. So most of what you see in the Azure number is broadly inferencing.

And Mark, I think it may be helpful to sort of think about, like what is the new workload in AI? The new workload in AI, obviously, in our case starts with one of the frontier -- I mean, starts with the Frontier model Azure OpenAI. But it's not just about just one model, right? So you first -- you take that model. You do RLHF. You may do some fine-tuning.

You do retrieval, which means you are sort of either heating some storage meter or you're heating some compute meters. And so to -- and by the way, you will also distill a large model to a small model, and that would be a training perhaps. But that's a small batch training that uses essentially inference infrastructure. So I think that's what's happening.

So you could even say these AI workloads themselves will have a lifecycle which is they'll get rebuilt, then there'll be continuously optimized over time. So that's sort of one side. And I think if I understand your question, what's happening with the traditional optimization, and I think last quarter we said. One, we're going to continue to have these cycles where people will build new workloads, they will optimize the workloads and then they'll start new workloads.

So I think that that's what we continue to see, but that period of massive, I'll call it, optimization only and no new workloads start, that I think has ended at this point. So what you're seeing is much more of that continuous cycle by customers, both whether it comes to AI or whether it comes to the traditional workloads.

Amy Hood -- Chief Financial Officer

No, maybe I'll just add just a few things there. I think whether you use the word lapping, these optimization comparables, or the comparables easing is all sort of the same thing, that we're getting to that point in H2. That's absolutely true. We'd like to talk about the contribution of AI, specifically for the reason Satya talked about, these are -- this is starting to see the application of AI at scale.

And we want to be able to show people this is how that point will work. It's inferencing workloads where people are expecting productivity gains, other benefits that grow revenue. And so I do think about those as those related. And ultimately, the TAM we go after is best sort of across both of those, both AI workload and I guess non-AI workload.

Although, to Satya's point, you need all of it.

Mark Moerdler -- AllianceBernstein -- Analyst

Perfect. Thank you very much for the deep answer.

Brett Iversen -- General Manager, Investor Relations

Thanks, Mark. Joe, next question, please.

Operator

Our next question comes from the line of Brent Thill with Jefferies. Please proceed.

Brent Thill -- Jefferies -- Analyst

Good afternoon. Amy, the margin improvement is pretty shocking the most considering the investments that you and Satya are putting in AI. I'm curious if you could just walk through, how this is possible and what you're seeing so far in some of the costs that you're trying to manage as you scale up AI.

Amy Hood -- Chief Financial Officer

Thanks, Brent. First of all, thanks for the question. The teams are obviously been hard at work on this topic. We do point out that in Q2 because of the impact of the charge a year ago, you're seeing larger margin improvement than I would say sort of a run-rate margin improvement.

So let me first say that. Secondly, the absolute margin improvement has also been very good and it speaks to, I think one of the things Satya talked about and I reiterated a bit, which is that we really to make sure we're making investments, we're making them and consistently across the tech stack. The tech stack we're building, no matter what team is on is inclusive of AI enablement. And so think about as building that consistency without needing to add a lot of resources to do that.

It's been a real pivot of our entire investment infrastructure to be working on this work and I think that's important because it means you're shifting to an AI-first position, not just in the language we use but in what people are working on day-to-day, that does obviously create leverage opportunity. There has also been really good work put in by many teams on improving the gross margin of the products, we talked about it was Office 365, we talked about in Azure core. We even talked about it across our devices portfolio, where we've seen material improvements over the course of the year. And so when you kind of take improvements at the gross margin level, plus this consistency of repivoting our workforce toward the AI-first work we're doing without adding material number of people to the workforce, you end up with that type of leverage.

And we still need to be investing. And so the important part, invest toward the thing that's going to shape the next decade and continue to stay focused on being able to deliver your day-to-day commitments. And so it's a great question. And hopefully, that helps piece apart a few of the components.

Brent Thill -- Jefferies -- Analyst

Thanks, Amy.

Brett Iversen -- General Manager, Investor Relations

Thanks, Brent. Joe, next question, please.

Operator

Our next question comes from the line of Kash Rangan with Goldman Sachs. Please proceed.

Kash Rangan -- Goldman Sachs -- Analyst

Hi. Thank you very much. A superb quarter of great improvements. One -- just one question for you, Satya.

Cloud computing changed the tech stack in ways that we could not imagine 10 years back, the nature of the database layer or the operating system layer, every layer just changed dramatically. How do you foresee generative AI changing the tech stack as we know it? Thank you so much.

Satya Nadella -- Chief Executive Officer

Yeah. I think it's going to have a very foundational impact. In fact, you can say the core compute architecture itself changes, everything from power, power density to the data center design, to what used to be the accelerator, now is the sort of the main CPU, so to speak, or the main compute unit. And so I think in the network, the memory architecture, all of it, so as the core computer architecture changes, I think every workload changes.

And so yes, so there is a full, like, take our data layer, the most exciting thing for me in the last year has been to see how our data layer has evolved to be built for AI, right? If you think about Fabric, one of the genius of Fabric is to be able to say, let's separate out storage from the compute layer, in compute we'll have traditional sequels that have Spark, and by the way, you can have an Azure AI job on-top of the same data lake, so to speak, or the lake house pattern. And then the business model you can combine all of those different compute. So that's the type of compute architecture. So it's sort of a -- so that's just one example.

The tool stuff is changing. Office, I mean, if you think about what -- if I look at Copilot; Copilot's extensibility with GPT, Copilot apps to the Copilot stack, that's another sort of part of what's happening to the tech stack. So yes, I mean, definitely builds. I mean.

I do believe, being in the cloud has been very helpful to build AI, but now, AI is just redefining what it means to have, what the cloud looks like, both at the infrastructure level and the app model.

Kash Rangan -- Goldman Sachs -- Analyst

Terrific. Thank you so much.

Brett Iversen -- General Manager, Investor Relations

Thanks, Kash. Joe, our next question, please.

Operator

Our next question comes from the line of Karl Keirstead with UBS. Please proceed.

Karl Keirstead -- UBS -- Analyst

Thank you. I wanted to return to AI, the 6-point AI lift to Azure is just extraordinary, but I wanted to ask you about your progress in standing up the infrastructure to meet that demand. If you feel like Microsoft is supply GPU-constrained. Is the success you've had maybe working through some of the scaling bottlenecks that some of the other cloud infrastructure providers have talked about, a little bit maybe on the infrastructure scaling front might be interesting.

Thank you.

Amy Hood -- Chief Financial Officer

Thanks, Karl. Maybe I'll start and Satya feel free to add on. I think we feel really good about where we have been in terms of adding capacity. You started to see the acceleration in our capital expense starting almost a year ago, and you've seen us scale through that process.

And that is going toward as we talked about Servers and also new data center footprints to be able to meet what we see as this demand and really changing demand as we look forward. And so I do feel like the team has done a very good job. I feel like, primarily obviously, this is being built by us, but we've also used third-party capacity to help when we could have that help us in terms of meeting customer demand. And I think looking forward, you'll tend to C&I guide toward it, accelerated capital expense to continue to be able to add capacity in the coming quarters, given what we see in terms of pipeline.

Brett Iversen -- General Manager, Investor Relations

Thanks, Karl. Joe, next question, please.

Operator

Our next question comes from the line of Brad Zelnick with Deutsche Bank. Please proceed.

Brad Zelnick -- Deutsche Bank -- Analyst

Great. Thank you so much for taking the question. The early market feedback that we're all hearing on Microsoft 365 Copilot is very powerful. Can you provide more granularity on what you're seeing in terms of adoption trends versus perhaps other new product introductions in the past, what if anything is holding it back, and how much of a priority is it to get it in the hands of customers? To what lengths might you go to incentivize just getting it out in the market? Thank you.

Satya Nadella -- Chief Executive Officer

No, thank you for the question, Brad. So a couple of things. In my comments I said increase in relation to our previous suites like, let's say, E3 or E5. Whatever two months in, it's definitely much faster than that.

And so from that perspective, it's exciting to see, I'd say, the demand signal, the deployment signal. I was looking at by tenant, even usage, it's faster than anything else because it's easier, right? I mean, it's sort of -- it shows up in your app, if you click on it, like any ribbon thing, and it becomes a daily habit. So it in fact, it reminds me a little bit of sort of the back in the day of PC adoption, right? It's kind of -- I think it first starts off with few people having access. There are many companies that are doing standard issue, right? So just like PCs became standard issue at some point after PCs being adopted by early adopters.

I think that's the cycle that at least we expect. In terms of what we're seeing, it's actually interesting, if you look at the data we have, summarization, that's what it's like, number one. Like I'm doing summarization of Teams meetings inside of Teams, during the meeting, after the meeting, Word documents summarization, I get something in email on summarizing. So summarization has become a big deal.

Drafts, right, you're drafting emails, drafting documents. So anytime you want to start something, the blank page thing goes away and you start by prompting and drafting. Chat, to me, the most powerful feature is now you have the most important database in your company, which happens to be the database of your documents and communications. It is now queryable by natural language in a powerful way, right? I can go and say, what are all the things Amy said I should be watching out for next quarter, and it will come out with great detail.

And so Chat, summarization, draft, and also, by the way, actions. One of the most used thing is here's the Word document, go complete, I mean, create a PowerPoint for me. So those are the stuff that is also beginning. So I feel like these all become -- but fundamentally, what happens is, if you remember the PC adoption cycle, what it did was work artifact and workflow changed, right? You can imagine what forecasting was before Excel and email and what it was after.

So similarly, you'll see work and workflow change as people summarize faster, draft regulatory submissions faster, chat to get knowledge from your business. And so those are the things that we are seeing as overall patterns.

Amy Hood -- Chief Financial Officer

And maybe just to add two points. One of the exciting things, as I said, for some companies, it's going to be standard issue like PC. For other companies, they may want to do a land with a smaller group, see the productivity gains, and then expand. And so being able to lift some of the seat requirements that we did earlier this month, it's really going to allow customers to be able to use that approach, too.

And the other thing I would add, we always talk about in enterprise software. You sell software, then you wait, and then it gets deployed. And then after deployment, you want to see usage. And in particular, what we've seen and you would expect this, in some ways with Copilot.

Even in the early stages, obviously, deployment happens very quickly. But really, what we're seeing is engagement grow. As to Satya's point on how you learn and your behavior changes, you see engagement grow with time. And so I think those are just to put a pin in that because it's an important dynamic when we think about the optimism you hear from us.

Brad Zelnick -- Deutsche Bank -- Analyst

Excellent. Thank you so much.

Brett Iversen -- General Manager, Investor Relations

Thanks, Brad. Joe, next question, please.

Operator

Our next question comes from the line of Mark Murphy with J.P. Morgan. Please proceed.

Mark Murphy -- JPMorgan Chase and Company -- Analyst

Yeah. Thank you very much. Is it possible to unpack the 6-point AI services tailwind, just to help us understand which elements ramped up by the 3 incremental points? For instance, is it more of the OpenAI inferencing, GitHub Copilot, other copilots, the Azure OpenAI service, third-party LLMs running on Azure? I'm just wondering, where did you see the strongest step up in that activity?

Amy Hood -- Chief Financial Officer

Mark, without getting into tons of line items, it's more simple to think of it as, really, it's people adopting it for inferencing at the API generally. I mean, that's the easiest way to think about it. And we also saw growth in GitHub Copilot which you talked -- which Satya talked about, and we saw a growing number of third parties using it in some small ways for training. But this is primarily an inferencing workload right now in terms of what's driving that number.

We used to think of it that way.

Satya Nadella -- Chief Executive Officer

Azure OpenAI and then OpenAIs on APIs on top of Azure would be the sort of the major drivers, but there is a lot of the small batch training that goes on, whether it's a graph or fine-tuning, and then lot of people who are starting to use models as a service with all the other new models, but it's predominantly Azure OpenAI today.

Mark Murphy -- JPMorgan Chase and Company -- Analyst

Thank you.

Brett Iversen -- General Manager, Investor Relations

Thanks, Mark. Joe, next question, please.

Operator

Our next question comes from the line of Brad Reback with Stifel. Please proceed.

Brad Reback -- Stifel Financial Corp. -- Analyst

Great. Thanks very much. Amy, for many, many years in commercial Office 365 seat growth has far outpaced ARPU and over the last couple of quarters, we're getting a convergence, obviously, as the seat count gets really large. As we look forward, should they run even for a period of time? Or should we expect ARPU to outpace seat growth here in the short term? Thanks.

Amy Hood -- Chief Financial Officer

That's a great question, Brad. Let me split apart the components. And then we can come back to whether they should equalize or just go on sort of a bit, actually believe it or not, somewhat independent trajectory and I will explain why I say that. Your seat growth as we talk about is primarily from, at this point, small and medium-sized businesses and really frontline workers scenarios.

And to your point on occasion, those are lower ARPU seats, but there are also -- there are new seats and so you see that in the seat count number. And as we get through and we've seen that come down a little bit quarter over quarter, and we've guided for that really to happen again next quarter. But a very separate thing is being able to add ARPU. And traditionally, and again this quarter, right, that's come over time from E3 then from E5.

And we're continuing to see very healthy seat momentum, and you heard very good renewals. So all of that, right, completely independent in some way from seat growth. Then the next thing that actually we just talked about, maybe in Brad's question, I'm trying to -- is that you're going to see Copilot revenue will run there as ARPU, right? That won't show a seat growth. So you'll have E3, E5 transition, Copilot all show up in ARPU over time, and then you'll have the seat growth be primarily still small business and frontline worker and maybe new industry scenarios.

So I tend to not really, Brad, think about them as related lines, believe it or not. I think about them as sort of unique independent motions we run, and there is still room for seat growth. And obviously, with the levers we've talked about, there's room for ARPU growth as well.

Brad Reback -- Stifel Financial Corp. -- Analyst

That's great. Thanks very much.

Brett Iversen -- General Manager, Investor Relations

Thanks, Brad. Joe, we have time for one last question.

Operator

Our last question will come from the line of Tyler Radke with Citi. Please proceed.

Tyler Radke -- Citi -- Analyst

Thanks for taking my question. Satya, your enthusiasm about GitHub Copilot was noticeable on the conference call and at the AI Summit in New York last week. I'm wondering how you're thinking about pricing. Obviously, this is driving pretty incredible breakthroughs and productivity for developers.

But how do you think about your ability to drive ARPU on the GitHub Copilot over time? And just talk us through how you're thinking about the next phase of new releases there.

Satya Nadella -- Chief Executive Officer

Yeah. I mean -- it's -- I always go back to sort of my own conviction that this generation of AI is going to be different, started with the move from 2.5 to 3 of GPT and then its use inside of developer scenario with GitHub Copilot. And so yes, I think this is the place where it's most evolved. In terms of its economic benefits or productivity benefits, and you see it.

We see it inside of Microsoft. We see it in all of the key studies we put out of customers. Everybody had talked to its pickup, but it is the one place where it's becoming standard issue for any developer is like if you take away spellcheck from Word, I'll be unemployable. And similarly, it will be like -- I think GitHub Copilot becomes core to anybody who is doing software development.

The thing that you brought up is a little bit of a continuation to how Amy talked about, right, so you are going to start seeing people think of these tools as productivity enhancers, right? I mean, if I look at it, our ARPUs have been great, but they're pretty low. Even though we've had a lot of success, it's not like we had a high-priced ARPU company. I think what you're going to start finding is, whether it's Sales Copilot or Service Copilot or GitHub Copilot or Security Copilot, they are going to fundamentally capture some of the value they drive in terms of the productivity of the opex, right? So it's like 2 points, 3 points of opex leverage would be goal is on software spend. I think that's a pretty straightforward value equation.

And so that's the first time, I mean, this is not something we've been able to make the case for before whereas now I think we have that case. Then even the horizontal copilot is what Amy was talking about, which is at the Office 365 or Microsoft 365 level, even there, you can make the same argument whatever ARPU we may even have with E5, now, you can see incrementally as a percentage of the opex, how much would you pay for a copilot to give you more time savings for example. And so yes, I think all up, I do see this as a new vector for us in what I'll call the next phase of knowledge work and frontline work, even in their productivity and how we participate. And I think GitHub Copilot, I never thought of the tools business as fundamentally participating in the operating expenses of a company's spend on, let's say, development activity and now you're seeing that transition.

It is just not tools. It's about productivity of your dev team.

Brett Iversen -- General Manager, Investor Relations

Thanks, Tyler. That wraps up the Q&A portion of today's earnings call. Thank you for joining us today, and we look forward to speaking with all of you soon.

Amy Hood -- Chief Financial Officer

Thank you.

Satya Nadella -- Chief Executive Officer

Thank you."
