{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from functions import get_tickers, get_stock_dict, get_companies_by_50\n",
    "QUARTERS = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
    "YEARS = [str(2005 + i) for i in range(18)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_tickers()\n",
    "dict_of_df = get_stock_dict()\n",
    "t_50, t_100, t_150, t_200, t_250 = get_companies_by_50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_texts = []\n",
    "final_labels = []\n",
    "bad_starts = [\"Operator:\", \"TRANSCRIPT\", \"Operator\", \"Related:\", \"Executives\"]\n",
    "\n",
    "#Iterates through the first 300 companies\n",
    "for i, tick in enumerate(tickers[:300]):\n",
    "    #print(i)\n",
    "    #Check to select the correct company transcript files (\"t_50\" variable)\n",
    "    if i < 50:\n",
    "        #Get company price and transcript date\n",
    "        company = t_50[tick]\n",
    "        company_price_df = dict_of_df[tick]\n",
    "        #Transform Date column so that the time of day is not included for price data (makes look up easier)\n",
    "        company_price_df[\"Date\"] = company_price_df[\"Date\"].map(lambda x: x.split()[0])\n",
    "        #Iterate through years\n",
    "        for year in YEARS:\n",
    "            #Check to see if a earnings call has been reported for the given year\n",
    "            if len(company[year].keys()) != 0:\n",
    "                #Iterate through all the quarters that have a released earnings call\n",
    "                for quarter in company[year].keys():\n",
    "                    #Grabs date and transcript of the earnings call\n",
    "                    date = company[year][quarter][\"date\"].split()\n",
    "                    transcript = company[year][quarter][\"transcript\"]\n",
    "\n",
    "                    #Checks to see when the call is released\n",
    "                    #If the call is in the middle of the day we want the close price of the previous day\n",
    "                    #If call is after hours we use the close price of that day\n",
    "                    time = date[1].split(\":\")\n",
    "                    date_0 = 0\n",
    "                    if int(time[0] + time[1]) < 1600:\n",
    "                        date_0 = (company_price_df[\"Date\"] == date[0]).shift(-1, fill_value = False)\n",
    "                        #print(\"first\", company_price_df[\"Date\"], date[0])\n",
    "                    else:\n",
    "                        date_0 = company_price_df[\"Date\"] == date[0]\n",
    "                        #print(\"second\", sum(date_0))\n",
    "                    if sum(date_0) !=0:\n",
    "                        #Grabs the Close price prior to earnings call, 20 days after, and 60 days after\n",
    "                        date_20 = date_0.shift(20, fill_value = False)\n",
    "                        date_60 = date_0.shift(60, fill_value = False)\n",
    "                        data_on_dates = company_price_df[date_0 + date_20 + date_60]\n",
    "                        close_price_0, close_price_20, close_price_60 = data_on_dates[\"Close\"]\n",
    "\n",
    "                        #Calculates the one month and three month price change\n",
    "                        one_month_change = ((close_price_20 - close_price_0) / close_price_0) * 100\n",
    "                        three_month_change = ((close_price_60 - close_price_0) / close_price_0) * 100\n",
    "\n",
    "                        #Creates a label for whether the stock has gone up, down, or stayed the same after the call release\n",
    "                        label = 0\n",
    "                        if one_month_change > 2 and three_month_change > 4:\n",
    "                            label = 1\n",
    "                        elif one_month_change < -2 and three_month_change < -4:\n",
    "                            label = -1\n",
    "\n",
    "                        sentences = transcript.split(\"\\n\")\n",
    "                        for sentence in sentences:\n",
    "                            check = sentence.split()\n",
    "                            check1 = sentence.split(\":\")\n",
    "                            if len(check) !=0 and len(check) < 513:\n",
    "                                if check[0] not in bad_starts and len(check1) == 2:\n",
    "                                    #print(sentence.split(\":\"))\n",
    "                                    final_texts.append(sentence.split(\":\")[1])\n",
    "                                    final_labels.append(label)\n",
    "    \"\"\"elif i < 100 and i >= 50:\n",
    "        #Get company price and transcript date\n",
    "        company = t_100[tick]\n",
    "        company_price_df = dict_of_df[tick]\n",
    "        #Transform Date column so that the time of day is not included for price data (makes look up easier)\n",
    "        company_price_df[\"Date\"] = company_price_df[\"Date\"].map(lambda x: x.split()[0])\n",
    "        #Iterate through years\n",
    "        for year in YEARS:\n",
    "            #Check to see if a earnings call has been reported for the given year\n",
    "            if len(company[year].keys()) != 0:\n",
    "                #Iterate through all the quarters that have a released earnings call\n",
    "                for quarter in company[year].keys():\n",
    "                    #Grabs date and transcript of the earnings call\n",
    "                    date = company[year][quarter][\"date\"].split()\n",
    "                    transcript = company[year][quarter][\"transcript\"]\n",
    "\n",
    "                    #Checks to see when the call is released\n",
    "                    #If the call is in the middle of the day we want the close price of the previous day\n",
    "                    #If call is after hours we use the close price of that day\n",
    "                    time = date[1].split(\":\")\n",
    "                    date_0 = 0\n",
    "                    if int(time[0] + time[1]) < 1600:\n",
    "                        date_0 = (company_price_df[\"Date\"] == date[0]).shift(-1, fill_value = False)\n",
    "                        #print(\"first\", company_price_df[\"Date\"], date[0])\n",
    "                    else:\n",
    "                        date_0 = company_price_df[\"Date\"] == date[0]\n",
    "                        #print(\"second\", sum(date_0))\n",
    "                    if sum(date_0) !=0:\n",
    "                        #Grabs the Close price prior to earnings call, 20 days after, and 60 days after\n",
    "                        date_20 = date_0.shift(20, fill_value = False)\n",
    "                        date_60 = date_0.shift(60, fill_value = False)\n",
    "                        data_on_dates = company_price_df[date_0 + date_20 + date_60]\n",
    "                        close_price_0, close_price_20, close_price_60 = data_on_dates[\"Close\"]\n",
    "\n",
    "                        #Calculates the one month and three month price change\n",
    "                        one_month_change = ((close_price_20 - close_price_0) / close_price_0) * 100\n",
    "                        three_month_change = ((close_price_60 - close_price_0) / close_price_0) * 100\n",
    "\n",
    "                        #Creates a label for whether the stock has gone up, down, or stayed the same after the call release\n",
    "                        label = 0\n",
    "                        if one_month_change > 5 and three_month_change > 10:\n",
    "                            label = 1\n",
    "                        elif one_month_change < -5 and three_month_change < -10:\n",
    "                            label = -1\n",
    "\n",
    "                        sentences = transcript.split(\"\\n\")\n",
    "                        for sentence in sentences:\n",
    "                            check = sentence.split()\n",
    "                            check1 = sentence.split(\":\")\n",
    "                            if len(check) !=0 and len(check) < 513:\n",
    "                                if check[0] not in bad_starts and len(check1) == 2:\n",
    "                                    #print(sentence.split(\":\"))\n",
    "                                    final_texts.append(sentence.split(\":\")[1])\n",
    "                                    final_labels.append(label)\n",
    "    elif i < 150 and i >= 100:\n",
    "        #Get company price and transcript date\n",
    "        company = t_150[tick]\n",
    "        company_price_df = dict_of_df[tick]\n",
    "        #Transform Date column so that the time of day is not included for price data (makes look up easier)\n",
    "        company_price_df[\"Date\"] = company_price_df[\"Date\"].map(lambda x: x.split()[0])\n",
    "        #Iterate through years\n",
    "        for year in YEARS:\n",
    "            #Check to see if a earnings call has been reported for the given year\n",
    "            if len(company[year].keys()) != 0:\n",
    "                #Iterate through all the quarters that have a released earnings call\n",
    "                for quarter in company[year].keys():\n",
    "                    #Grabs date and transcript of the earnings call\n",
    "                    date = company[year][quarter][\"date\"].split()\n",
    "                    transcript = company[year][quarter][\"transcript\"]\n",
    "\n",
    "                    #Checks to see when the call is released\n",
    "                    #If the call is in the middle of the day we want the close price of the previous day\n",
    "                    #If call is after hours we use the close price of that day\n",
    "                    time = date[1].split(\":\")\n",
    "                    date_0 = 0\n",
    "                    if int(time[0] + time[1]) < 1600:\n",
    "                        date_0 = (company_price_df[\"Date\"] == date[0]).shift(-1, fill_value = False)\n",
    "                        #print(\"first\", company_price_df[\"Date\"], date[0])\n",
    "                    else:\n",
    "                        date_0 = company_price_df[\"Date\"] == date[0]\n",
    "                        #print(\"second\", sum(date_0))\n",
    "                    if sum(date_0) !=0:\n",
    "                        #Grabs the Close price prior to earnings call, 20 days after, and 60 days after\n",
    "                        date_20 = date_0.shift(20, fill_value = False)\n",
    "                        date_60 = date_0.shift(60, fill_value = False)\n",
    "                        data_on_dates = company_price_df[date_0 + date_20 + date_60]\n",
    "                        close_price_0, close_price_20, close_price_60 = data_on_dates[\"Close\"]\n",
    "\n",
    "                        #Calculates the one month and three month price change\n",
    "                        one_month_change = ((close_price_20 - close_price_0) / close_price_0) * 100\n",
    "                        three_month_change = ((close_price_60 - close_price_0) / close_price_0) * 100\n",
    "\n",
    "                        #Creates a label for whether the stock has gone up, down, or stayed the same after the call release\n",
    "                        label = 0\n",
    "                        if one_month_change > 5 and three_month_change > 10:\n",
    "                            label = 1\n",
    "                        elif one_month_change < -5 and three_month_change < -10:\n",
    "                            label = -1\n",
    "\n",
    "                        sentences = transcript.split(\"\\n\")\n",
    "                        for sentence in sentences:\n",
    "                            check = sentence.split()\n",
    "                            check1 = sentence.split(\":\")\n",
    "                            if len(check) !=0 and len(check) < 513:\n",
    "                                if check[0] not in bad_starts and len(check1) == 2:\n",
    "                                    #print(sentence.split(\":\"))\n",
    "                                    final_texts.append(sentence.split(\":\")[1])\n",
    "                                    final_labels.append(label)\n",
    "    elif i < 200 and i >= 150:\n",
    "        #Get company price and transcript date\n",
    "        company = t_200[tick]\n",
    "        company_price_df = dict_of_df[tick]\n",
    "        #Transform Date column so that the time of day is not included for price data (makes look up easier)\n",
    "        company_price_df[\"Date\"] = company_price_df[\"Date\"].map(lambda x: x.split()[0])\n",
    "        #Iterate through years\n",
    "        for year in YEARS:\n",
    "            #Check to see if a earnings call has been reported for the given year\n",
    "            if len(company[year].keys()) != 0:\n",
    "                #Iterate through all the quarters that have a released earnings call\n",
    "                for quarter in company[year].keys():\n",
    "                    #Grabs date and transcript of the earnings call\n",
    "                    date = company[year][quarter][\"date\"].split()\n",
    "                    transcript = company[year][quarter][\"transcript\"]\n",
    "\n",
    "                    #Checks to see when the call is released\n",
    "                    #If the call is in the middle of the day we want the close price of the previous day\n",
    "                    #If call is after hours we use the close price of that day\n",
    "                    time = date[1].split(\":\")\n",
    "                    date_0 = 0\n",
    "                    if int(time[0] + time[1]) < 1600:\n",
    "                        date_0 = (company_price_df[\"Date\"] == date[0]).shift(-1, fill_value = False)\n",
    "                        #print(\"first\", company_price_df[\"Date\"], date[0])\n",
    "                    else:\n",
    "                        date_0 = company_price_df[\"Date\"] == date[0]\n",
    "                        #print(\"second\", sum(date_0))\n",
    "                    if sum(date_0) !=0:\n",
    "                        #Grabs the Close price prior to earnings call, 20 days after, and 60 days after\n",
    "                        date_20 = date_0.shift(20, fill_value = False)\n",
    "                        date_60 = date_0.shift(60, fill_value = False)\n",
    "                        data_on_dates = company_price_df[date_0 + date_20 + date_60]\n",
    "                        close_price_0, close_price_20, close_price_60 = data_on_dates[\"Close\"]\n",
    "\n",
    "                        #Calculates the one month and three month price change\n",
    "                        one_month_change = ((close_price_20 - close_price_0) / close_price_0) * 100\n",
    "                        three_month_change = ((close_price_60 - close_price_0) / close_price_0) * 100\n",
    "\n",
    "                        #Creates a label for whether the stock has gone up, down, or stayed the same after the call release\n",
    "                        label = 0\n",
    "                        if one_month_change > 5 and three_month_change > 10:\n",
    "                            label = 1\n",
    "                        elif one_month_change < -5 and three_month_change < -10:\n",
    "                            label = -1\n",
    "\n",
    "                        sentences = transcript.split(\"\\n\")\n",
    "                        for sentence in sentences:\n",
    "                            check = sentence.split()\n",
    "                            check1 = sentence.split(\":\")\n",
    "                            if len(check) !=0 and len(check) < 513:\n",
    "                                if check[0] not in bad_starts and len(check1) == 2:\n",
    "                                    #print(sentence.split(\":\"))\n",
    "                                    final_texts.append(sentence.split(\":\")[1])\n",
    "                                    final_labels.append(label)\n",
    "    elif i < 250 and i >= 200:\n",
    "        #Get company price and transcript date\n",
    "        company = t_250[tick]\n",
    "        company_price_df = dict_of_df[tick]\n",
    "        #Transform Date column so that the time of day is not included for price data (makes look up easier)\n",
    "        company_price_df[\"Date\"] = company_price_df[\"Date\"].map(lambda x: x.split()[0])\n",
    "        #Iterate through years\n",
    "        for year in YEARS:\n",
    "            #Check to see if a earnings call has been reported for the given year\n",
    "            if len(company[year].keys()) != 0:\n",
    "                #Iterate through all the quarters that have a released earnings call\n",
    "                for quarter in company[year].keys():\n",
    "                    #Grabs date and transcript of the earnings call\n",
    "                    date = company[year][quarter][\"date\"].split()\n",
    "                    transcript = company[year][quarter][\"transcript\"]\n",
    "\n",
    "                    #Checks to see when the call is released\n",
    "                    #If the call is in the middle of the day we want the close price of the previous day\n",
    "                    #If call is after hours we use the close price of that day\n",
    "                    time = date[1].split(\":\")\n",
    "                    date_0 = 0\n",
    "                    if int(time[0] + time[1]) < 1600:\n",
    "                        date_0 = (company_price_df[\"Date\"] == date[0]).shift(-1, fill_value = False)\n",
    "                        #print(\"first\", company_price_df[\"Date\"], date[0])\n",
    "                    else:\n",
    "                        date_0 = company_price_df[\"Date\"] == date[0]\n",
    "                        #print(\"second\", sum(date_0))\n",
    "                    if sum(date_0) !=0:\n",
    "                        #Grabs the Close price prior to earnings call, 20 days after, and 60 days after\n",
    "                        date_20 = date_0.shift(20, fill_value = False)\n",
    "                        date_60 = date_0.shift(60, fill_value = False)\n",
    "                        data_on_dates = company_price_df[date_0 + date_20 + date_60]\n",
    "                        close_price_0, close_price_20, close_price_60 = data_on_dates[\"Close\"]\n",
    "\n",
    "                        #Calculates the one month and three month price change\n",
    "                        one_month_change = ((close_price_20 - close_price_0) / close_price_0) * 100\n",
    "                        three_month_change = ((close_price_60 - close_price_0) / close_price_0) * 100\n",
    "\n",
    "                        #Creates a label for whether the stock has gone up, down, or stayed the same after the call release\n",
    "                        label = 0\n",
    "                        if one_month_change > 5 and three_month_change > 10:\n",
    "                            label = 1\n",
    "                        elif one_month_change < -5 and three_month_change < -10:\n",
    "                            label = -1\n",
    "                        \n",
    "\n",
    "                        sentences = transcript.split(\"\\n\")\n",
    "                        for sentence in sentences:\n",
    "                            check = sentence.split()\n",
    "                            check1 = sentence.split(\":\")\n",
    "                            if len(check) !=0 and len(check) < 513:\n",
    "                                if check[0] not in bad_starts and len(check1) == 2:\n",
    "                                    #print(sentence.split(\":\"))\n",
    "                                    final_texts.append(sentence.split(\":\")[1])\n",
    "                                    final_labels.append(label)\n",
    "\"\"\"\n",
    "\n",
    "final_df = pd.DataFrame({\"Text\": final_texts, \"Labels\": final_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of data labeled 0: 0.5219018434385226\n",
      "Proportion of data labeled 1: 0.32811924425960826\n",
      "Proportion of data labeled -1: 0.14997891230186908\n",
      "187313\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of data labeled 0:\", final_df[final_df[\"Labels\"] == 0].shape[0] / final_df.shape[0])\n",
    "print(\"Proportion of data labeled 1:\", final_df[final_df[\"Labels\"] == 1].shape[0] / final_df.shape[0])\n",
    "print(\"Proportion of data labeled -1:\", final_df[final_df[\"Labels\"] == -1].shape[0] / final_df.shape[0])\n",
    "print(final_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93656\n",
      "Proportion of data labeled 0: 0.7563637140172547\n",
      "Proportion of data labeled 1: 0.1696314171008798\n",
      "Proportion of data labeled -1: 0.07400486888186555\n"
     ]
    }
   ],
   "source": [
    "temp = final_df.sample(frac=.5).reset_index(drop=True)\n",
    "print(temp.shape[0])\n",
    "print(\"Proportion of data labeled 0:\", temp[temp[\"Labels\"] == 0].shape[0] / temp.shape[0])\n",
    "print(\"Proportion of data labeled 1:\", temp[temp[\"Labels\"] == 1].shape[0] / temp.shape[0])\n",
    "print(\"Proportion of data labeled -1:\", temp[temp[\"Labels\"] == -1].shape[0] / temp.shape[0])\n",
    "X_train = temp[\"Text\"]\n",
    "y_train = temp[\"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameshetherington/anaconda3/envs/ml-0451/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Done\n",
      "Step 1 Done\n",
      "Step 2 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/jameshetherington/anaconda3/envs/ml-0451/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 Done\n",
      "Step 4 Done\n",
      "prebatch\n",
      "Batch  1\n",
      "Batch:  (tensor([[  101,  3100,  1012,  ...,     0,     0,     0],\n",
      "        [  101,  2307,  1012,  ...,     0,     0,     0],\n",
      "        [  101, 15419,  1011,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  3100,  1012,  ...,     0,     0,     0],\n",
      "        [  101,  3100,  1012,  ...,     0,     0,     0],\n",
      "        [  101, 27547,  1012,  ...,     0,     0,     0]]), tensor([0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(1.2008, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3802, -0.1359, -0.1424],\n",
      "        [-0.3429,  0.1051, -0.5226],\n",
      "        [-0.3032,  0.0394, -0.5149],\n",
      "        [-0.2540,  0.0265, -0.2626],\n",
      "        [-0.5119, -0.0565, -0.2213],\n",
      "        [-0.4503, -0.0849, -0.4769],\n",
      "        [-0.3489, -0.1429, -0.2031],\n",
      "        [-0.3221, -0.0482, -0.0891],\n",
      "        [-0.2076, -0.1168, -0.1472],\n",
      "        [-0.2262,  0.0478, -0.4269],\n",
      "        [-0.4417, -0.0802, -0.5041],\n",
      "        [-0.2782, -0.1394, -0.5047],\n",
      "        [-0.1087, -0.2532, -0.3286],\n",
      "        [-0.5649,  0.0431, -0.3715],\n",
      "        [-0.2783, -0.1263, -0.6812],\n",
      "        [-0.4597, -0.1569, -0.3236],\n",
      "        [-0.3416, -0.3140, -0.7563],\n",
      "        [-0.4651,  0.0367, -0.3156],\n",
      "        [-0.3674,  0.0235, -0.5493],\n",
      "        [-0.5657,  0.1603, -0.8344],\n",
      "        [-0.4066,  0.1222, -0.6013],\n",
      "        [-0.4107, -0.2237, -0.3306],\n",
      "        [-0.3965, -0.0738, -0.4144],\n",
      "        [-0.4150,  0.0413, -0.2449],\n",
      "        [-0.5283, -0.2489, -0.3097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  2\n",
      "Batch:  (tensor([[ 101, 2092, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2216, 2097,  ...,    0,    0,    0],\n",
      "        [ 101, 4283, 1010,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2279, 3160,  ...,    0,    0,    0],\n",
      "        [ 101, 2061, 2008,  ...,    0,    0,    0],\n",
      "        [ 101, 2157, 3398,  ...,    0,    0,    0]]), tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.5100, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9614, -1.0899, -0.8218],\n",
      "        [ 0.7428, -1.2280, -0.8710],\n",
      "        [ 0.4901, -1.1274, -0.4898],\n",
      "        [ 0.6342, -0.9938, -0.7879],\n",
      "        [ 0.8028, -1.0972, -0.9305],\n",
      "        [ 0.8973, -0.9522, -0.7731],\n",
      "        [ 0.6670, -1.0530, -0.7571],\n",
      "        [ 0.8564, -1.0428, -0.7222],\n",
      "        [ 0.7155, -0.8494, -0.9896],\n",
      "        [ 0.9146, -1.1923, -0.7981],\n",
      "        [ 0.7419, -1.1660, -0.6874],\n",
      "        [ 1.0449, -0.5466, -1.0700],\n",
      "        [ 0.8193, -0.8625, -0.4371],\n",
      "        [ 0.9129, -1.0288, -0.9391],\n",
      "        [ 0.7700, -1.1605, -1.0734],\n",
      "        [ 0.8838, -0.9946, -1.2796],\n",
      "        [ 1.0161, -1.1115, -1.1095],\n",
      "        [ 1.0146, -1.3323, -0.7582],\n",
      "        [ 0.7250, -1.2706, -0.6023],\n",
      "        [ 0.7668, -0.7579, -0.6296],\n",
      "        [ 0.8780, -1.2332, -0.7049],\n",
      "        [ 0.7962, -0.9084, -0.7962],\n",
      "        [ 0.7881, -1.1724, -0.8564],\n",
      "        [ 0.5691, -1.0318, -0.9549],\n",
      "        [ 0.6893, -1.1956, -1.1206]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  3\n",
      "Batch:  (tensor([[ 101, 2035, 2157,  ...,    0,    0,    0],\n",
      "        [ 101, 2307, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 9120, 2009,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 7632, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2092, 2651,  ...,    0,    0,    0],\n",
      "        [ 101, 4283, 1010,  ...,    0,    0,    0]]), tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.9736, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.2856, -1.9257, -1.2989],\n",
      "        [ 2.4002, -1.8684, -1.5657],\n",
      "        [ 2.4761, -1.7741, -1.2776],\n",
      "        [ 2.5549, -2.0974, -1.6586],\n",
      "        [ 2.4812, -2.0297, -1.6010],\n",
      "        [ 2.2724, -1.7376, -1.4186],\n",
      "        [ 2.5963, -1.6983, -1.1977],\n",
      "        [ 2.6780, -2.0846, -1.4513],\n",
      "        [ 2.4474, -1.6883, -1.6691],\n",
      "        [ 1.9979, -1.9379, -1.1571],\n",
      "        [ 2.6390, -2.1510, -1.5421],\n",
      "        [ 2.2079, -2.1828, -1.6826],\n",
      "        [ 2.5595, -1.9327, -1.2923],\n",
      "        [ 2.4135, -1.8374, -1.5086],\n",
      "        [ 2.4732, -1.9139, -1.3510],\n",
      "        [ 2.6362, -1.9866, -1.4255],\n",
      "        [ 2.4232, -2.0130, -1.4097],\n",
      "        [ 2.2559, -2.0595, -1.4349],\n",
      "        [ 2.6474, -2.0335, -1.5032],\n",
      "        [ 2.6452, -2.0594, -1.3516],\n",
      "        [ 2.6379, -2.0914, -1.6224],\n",
      "        [ 2.3317, -1.8748, -1.3901],\n",
      "        [ 2.6123, -1.9840, -1.3848],\n",
      "        [ 2.4210, -1.7841, -1.4743],\n",
      "        [ 2.4801, -1.9067, -1.4724]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  4\n",
      "Batch:  (tensor([[ 101, 4067, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2004, 2057,  ...,    0,    0,    0],\n",
      "        [ 101, 2053, 1012,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1998, 2074,  ...,    0,    0,    0],\n",
      "        [ 101, 2053, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 4283, 2153,  ...,    0,    0,    0]]), tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        2]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(2.8553, grad_fn=<NllLossBackward0>), logits=tensor([[-2.2724,  1.9852,  2.0831],\n",
      "        [-1.6949,  1.1863,  1.2303],\n",
      "        [-2.2246,  1.7839,  1.7351],\n",
      "        [-1.8819,  1.9026,  1.7595],\n",
      "        [-1.8142,  1.2780,  1.3115],\n",
      "        [-2.0131,  1.6146,  1.6834],\n",
      "        [-2.1724,  2.1254,  1.9815],\n",
      "        [-2.4145,  1.9657,  2.1516],\n",
      "        [-1.2316,  0.6999,  0.7868],\n",
      "        [-1.6234,  0.8819,  0.7751],\n",
      "        [-2.0123,  1.9356,  2.0033],\n",
      "        [-2.1042,  1.7053,  1.8799],\n",
      "        [-2.2303,  1.8604,  1.7359],\n",
      "        [-0.6655, -0.1286,  0.2859],\n",
      "        [-0.3997, -0.1375,  0.3604],\n",
      "        [-2.0977,  2.2812,  2.0218],\n",
      "        [-2.1044,  1.6549,  1.6298],\n",
      "        [-2.1949,  1.9101,  1.7330],\n",
      "        [-1.0006,  0.6134,  0.6181],\n",
      "        [-2.1569,  1.9915,  2.1802],\n",
      "        [-1.9578,  1.8529,  2.1452],\n",
      "        [-2.2584,  1.7644,  1.6646],\n",
      "        [-2.3996,  1.7400,  2.0460],\n",
      "        [-1.9873,  1.7905,  2.1371],\n",
      "        [-2.1797,  1.8584,  1.6998]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  5\n",
      "Batch:  (tensor([[ 101, 6945, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2061, 1045,  ...,    0,    0,    0],\n",
      "        [ 101, 2307, 1012,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2092, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2017, 2113,  ...,    0,    0,    0],\n",
      "        [ 101, 2307, 1012,  ...,    0,    0,    0]]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(1.0849, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3443, -0.3969,  0.4389],\n",
      "        [ 0.1070, -0.3614,  0.7381],\n",
      "        [ 0.4582, -0.4068,  0.5548],\n",
      "        [ 0.2517, -0.1115,  0.7493],\n",
      "        [ 0.6975, -0.6400,  0.4477],\n",
      "        [ 0.3123, -0.2614,  0.7714],\n",
      "        [ 0.4056, -0.4720,  0.4938],\n",
      "        [ 0.2448, -0.4937,  0.4188],\n",
      "        [ 0.3816, -0.6749,  0.7700],\n",
      "        [ 0.5355, -0.2566,  0.7026],\n",
      "        [ 0.4102, -0.4145,  0.7635],\n",
      "        [ 0.2315, -0.4458,  0.5043],\n",
      "        [ 0.5994, -0.6089,  0.6900],\n",
      "        [ 0.6483, -0.6516,  0.8819],\n",
      "        [ 0.4666, -0.1234,  0.4517],\n",
      "        [ 0.4556, -0.3852,  0.6451],\n",
      "        [ 0.3945, -0.3976,  0.8124],\n",
      "        [ 0.5393, -0.4966,  0.4479],\n",
      "        [ 0.5012, -0.2175,  0.6190],\n",
      "        [ 0.4666, -0.5164,  0.5783],\n",
      "        [ 0.2384, -0.2090,  0.8137],\n",
      "        [ 0.4370, -0.4563,  0.6519],\n",
      "        [ 0.5591, -0.4079,  0.6767],\n",
      "        [ 0.3828, -0.4276,  0.7655],\n",
      "        [ 0.6800, -0.3019,  0.8798]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  6\n",
      "Batch:  (tensor([[ 101, 2092, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 4067, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 4283, 1998,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2057, 2156,  ...,    0,    0,    0],\n",
      "        [ 101, 9805, 2361,  ...,    0,    0,    0],\n",
      "        [ 101, 3100, 1010,  ...,    0,    0,    0]]), tensor([0, 2, 2, 1, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 1, 0, 0, 2, 2, 0, 0, 2, 1, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(1.5073, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0435, -2.4825, -0.0148],\n",
      "        [ 1.9870, -2.5827,  0.0872],\n",
      "        [ 2.2733, -2.4846, -0.2353],\n",
      "        [ 1.8401, -2.2801, -0.0635],\n",
      "        [ 2.1638, -2.5362, -0.0856],\n",
      "        [ 1.7528, -2.6340,  0.0973],\n",
      "        [ 1.9360, -2.3773, -0.0682],\n",
      "        [ 1.8069, -2.3141, -0.0681],\n",
      "        [ 1.8253, -2.4813, -0.2075],\n",
      "        [ 2.1386, -2.4611,  0.0953],\n",
      "        [ 1.7262, -2.3776,  0.0328],\n",
      "        [ 1.6884, -2.3777, -0.0027],\n",
      "        [ 1.7344, -2.3082,  0.1947],\n",
      "        [ 2.0094, -2.4867, -0.3061],\n",
      "        [ 2.1656, -2.5326,  0.1013],\n",
      "        [ 2.0595, -2.4020,  0.0462],\n",
      "        [ 2.0527, -2.4551, -0.0393],\n",
      "        [ 2.0630, -2.4217,  0.0797],\n",
      "        [ 1.9665, -2.3379,  0.0488],\n",
      "        [ 1.8634, -2.2897,  0.2032],\n",
      "        [ 1.8186, -2.5475,  0.0952],\n",
      "        [ 2.1423, -2.5156, -0.0096],\n",
      "        [ 1.6956, -2.1825,  0.0533],\n",
      "        [ 1.6922, -2.5312,  0.0117],\n",
      "        [ 1.4414, -2.3478,  0.0590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  7\n",
      "Batch:  (tensor([[ 101, 6581, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 3398, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 2469, 1012,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1037, 3160,  ...,    0,    0,    0],\n",
      "        [ 101, 4067, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2469, 1010,  ...,    0,    0,    0]]), tensor([0, 0, 0, 2, 2, 2, 1, 0, 2, 0, 2, 2, 0, 0, 1, 0, 0, 2, 0, 1, 0, 1, 0, 0,\n",
      "        2]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(1.2421, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6325, -1.5482, -0.1237],\n",
      "        [ 1.4779, -1.6820, -0.0180],\n",
      "        [ 1.4099, -1.9429, -0.1361],\n",
      "        [ 1.4337, -1.9182,  0.0419],\n",
      "        [ 1.4907, -1.7994, -0.0405],\n",
      "        [ 1.3149, -1.6835,  0.0665],\n",
      "        [ 1.4568, -1.7482, -0.0974],\n",
      "        [ 1.5203, -1.5886, -0.1390],\n",
      "        [ 1.4713, -2.1481, -0.0934],\n",
      "        [ 1.4565, -1.9481, -0.0565],\n",
      "        [ 1.5301, -2.0765, -0.2202],\n",
      "        [ 1.5497, -1.8034,  0.0549],\n",
      "        [ 1.2020, -1.8437, -0.0478],\n",
      "        [ 1.5816, -2.0349,  0.0489],\n",
      "        [ 1.6141, -1.6168,  0.2046],\n",
      "        [ 1.4354, -1.9788, -0.0906],\n",
      "        [ 1.4870, -1.8604,  0.2411],\n",
      "        [ 1.6455, -1.8744, -0.1279],\n",
      "        [ 1.4152, -1.6899,  0.1365],\n",
      "        [ 1.5582, -1.8220, -0.0785],\n",
      "        [ 1.6774, -1.8079, -0.2139],\n",
      "        [ 1.5426, -1.9671, -0.0530],\n",
      "        [ 1.6257, -1.9896, -0.1050],\n",
      "        [ 1.6865, -2.0433,  0.1022],\n",
      "        [ 1.3570, -1.7473,  0.0608]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  8\n",
      "Batch:  (tensor([[ 101, 4067, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2079, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2168, 6304,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 8226, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 3100, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 3782, 1010,  ...,    0,    0,    0]]), tensor([0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.7541, grad_fn=<NllLossBackward0>), logits=tensor([[ 8.2168e-01, -1.0577e+00,  7.4826e-02],\n",
      "        [ 8.4453e-01, -1.0160e+00, -1.5128e-02],\n",
      "        [ 9.0690e-01, -1.1928e+00,  1.7696e-01],\n",
      "        [ 6.0231e-01, -1.3388e+00,  1.2791e-01],\n",
      "        [ 1.0294e+00, -9.4491e-01, -5.7785e-02],\n",
      "        [ 8.7394e-01, -9.0634e-01,  1.8975e-01],\n",
      "        [ 1.0088e+00, -1.0782e+00,  7.0131e-02],\n",
      "        [ 5.9212e-01, -9.7304e-01,  7.8217e-04],\n",
      "        [ 7.4533e-01, -1.1741e+00, -9.0637e-02],\n",
      "        [ 8.5658e-01, -9.8867e-01,  5.8019e-03],\n",
      "        [ 6.9976e-01, -8.0185e-01, -2.2205e-01],\n",
      "        [ 8.3421e-01, -1.1151e+00,  2.2441e-02],\n",
      "        [ 7.1588e-01, -8.8134e-01,  2.1006e-01],\n",
      "        [ 6.8273e-01, -1.3227e+00, -2.2539e-02],\n",
      "        [ 5.6673e-01, -1.1317e+00,  3.8064e-02],\n",
      "        [ 3.4772e-01, -8.7371e-01,  2.8011e-01],\n",
      "        [ 7.6066e-01, -1.0595e+00, -2.0818e-01],\n",
      "        [ 6.7286e-01, -9.7353e-01,  1.3967e-01],\n",
      "        [ 8.5950e-01, -8.1949e-01, -3.6121e-02],\n",
      "        [ 9.4103e-01, -1.0017e+00, -1.8919e-02],\n",
      "        [ 8.7670e-01, -1.0590e+00, -4.8569e-02],\n",
      "        [ 8.4047e-01, -9.9790e-01,  2.0376e-01],\n",
      "        [ 9.8106e-01, -1.0722e+00,  8.6292e-02],\n",
      "        [ 1.1204e+00, -1.0381e+00,  7.9270e-02],\n",
      "        [ 7.3959e-01, -9.9448e-01,  3.7519e-02]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  9\n",
      "Batch:  (tensor([[  101,  2292,  2033,  ...,     0,     0,     0],\n",
      "        [  101, 27547,  1012,  ...,     0,     0,     0],\n",
      "        [  101,  4283,  1010,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2167,  2637,  ...,     0,     0,     0],\n",
      "        [  101,  2204,  2851,  ...,     0,     0,     0],\n",
      "        [  101,  2061,  2057,  ...,     0,     0,     0]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 1, 2, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.7522, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7126, -0.6184, -0.2063],\n",
      "        [ 0.4845, -0.7067, -0.1736],\n",
      "        [ 0.6357, -0.4429, -0.2414],\n",
      "        [ 0.6721, -0.6236, -0.1262],\n",
      "        [ 0.8957, -0.7866, -0.2345],\n",
      "        [ 0.6688, -0.6531, -0.3128],\n",
      "        [ 0.7592, -0.5037, -0.3686],\n",
      "        [ 0.3561, -0.4911, -0.2902],\n",
      "        [ 0.5959, -0.8460,  0.1304],\n",
      "        [ 0.4682, -0.7163, -0.1817],\n",
      "        [ 0.8467, -0.4399, -0.1427],\n",
      "        [ 0.5928, -0.6834, -0.2242],\n",
      "        [ 0.6496, -0.8565, -0.2423],\n",
      "        [ 0.8634, -0.4804, -0.2313],\n",
      "        [ 0.6233, -0.7134, -0.0884],\n",
      "        [ 0.7415, -0.6223, -0.0245],\n",
      "        [ 0.4631, -0.4991, -0.2026],\n",
      "        [ 0.6581, -0.4728, -0.0508],\n",
      "        [ 0.6626, -0.7751, -0.1669],\n",
      "        [ 0.5372, -0.4928, -0.2320],\n",
      "        [ 0.7041, -0.7798, -0.0551],\n",
      "        [ 0.5430, -0.7018, -0.3242],\n",
      "        [ 0.6033, -0.7272, -0.3822],\n",
      "        [ 0.4130, -0.5449, -0.3215],\n",
      "        [ 0.7799, -0.5711, -0.0959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  10\n",
      "Batch:  (tensor([[ 101, 2092, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2204, 2851,  ...,    0,    0,    0],\n",
      "        [ 101, 2008, 1521,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2057, 1521,  ...,    0,    0,    0],\n",
      "        [ 101, 7632, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2748, 1010,  ...,    0,    0,    0]]), tensor([2, 0, 1, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.9701, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9875, -0.6860, -0.6832],\n",
      "        [ 0.9047, -0.7894, -0.2363],\n",
      "        [ 1.3613, -0.5564, -0.6258],\n",
      "        [ 1.0772, -0.8743, -0.6345],\n",
      "        [ 0.8421, -1.0100, -0.8227],\n",
      "        [ 0.9601, -0.6364, -0.7143],\n",
      "        [ 1.0073, -0.9258, -0.6530],\n",
      "        [ 1.1215, -0.6414, -0.7693],\n",
      "        [ 0.9110, -0.4563, -0.5374],\n",
      "        [ 1.3010, -0.8214, -0.3491],\n",
      "        [ 0.9996, -0.6548, -0.5593],\n",
      "        [ 1.1726, -0.5844, -0.8638],\n",
      "        [ 0.9177, -0.5734, -0.6989],\n",
      "        [ 1.0551, -0.4523, -0.8049],\n",
      "        [ 1.1810, -0.4817, -0.6049],\n",
      "        [ 0.9433, -0.4628, -0.4198],\n",
      "        [ 0.8709, -0.6438, -0.6573],\n",
      "        [ 0.9109, -0.3857, -0.7276],\n",
      "        [ 0.7651, -0.5247, -0.5681],\n",
      "        [ 1.0124, -0.6949, -0.8116],\n",
      "        [ 1.0019, -0.8226, -0.7013],\n",
      "        [ 1.0263, -0.9785, -0.5910],\n",
      "        [ 0.9026, -0.6192, -0.7240],\n",
      "        [ 0.8726, -0.4917, -0.4783],\n",
      "        [ 1.0929, -0.6616, -0.6396]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  11\n",
      "Batch:  (tensor([[ 101, 2748, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 2017, 2052,  ...,    0,    0,    0],\n",
      "        [ 101, 2053, 1012,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2017, 1005,  ...,    0,    0,    0],\n",
      "        [ 101, 2079, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2008, 2003,  ...,    0,    0,    0]]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.6022, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2266, -0.4129, -1.2241],\n",
      "        [ 1.3963, -0.3951, -0.9837],\n",
      "        [ 1.3889, -0.6616, -0.8210],\n",
      "        [ 1.3482, -0.5418, -0.7782],\n",
      "        [ 1.2441, -0.3320, -1.0613],\n",
      "        [ 1.1570, -0.3689, -0.8007],\n",
      "        [ 1.2998, -0.4981, -0.8758],\n",
      "        [ 1.0304, -0.5516, -0.8901],\n",
      "        [ 1.4162, -0.5029, -0.9396],\n",
      "        [ 1.1286, -0.2315, -0.6369],\n",
      "        [ 1.1468, -0.5675, -1.0505],\n",
      "        [ 1.0251, -0.2155, -0.7312],\n",
      "        [ 1.0642, -0.3388, -0.8234],\n",
      "        [ 1.1444, -0.7181, -0.8740],\n",
      "        [ 1.0925, -0.4640, -1.0010],\n",
      "        [ 1.5926, -0.5559, -0.6649],\n",
      "        [ 1.2392, -0.3860, -1.0857],\n",
      "        [ 1.3285, -0.5060, -0.8086],\n",
      "        [ 1.2685, -0.5325, -0.4618],\n",
      "        [ 1.0158, -0.5059, -0.8721],\n",
      "        [ 1.3043, -0.7340, -1.1816],\n",
      "        [ 1.1422, -0.6466, -1.0402],\n",
      "        [ 1.1042, -0.4614, -0.7262],\n",
      "        [ 0.9870, -0.2585, -0.8915],\n",
      "        [ 1.2315, -0.6653, -0.9252]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  12\n",
      "Batch:  (tensor([[ 101, 1037, 3232,  ...,    0,    0,    0],\n",
      "        [ 101, 1998, 1045,  ...,    0,    0,    0],\n",
      "        [ 101, 2748, 1010,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2053, 3976,  ...,    0,    0,    0],\n",
      "        [ 101, 1998, 2065,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2228,  ...,    0,    0,    0]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.3757, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7376, -0.4204, -1.2250],\n",
      "        [ 1.4954, -0.3551, -1.2192],\n",
      "        [ 1.2820, -0.2727, -1.4337],\n",
      "        [ 1.2392, -0.3754, -1.1833],\n",
      "        [ 1.4260, -0.3518, -1.4177],\n",
      "        [ 1.4086, -0.2610, -1.2132],\n",
      "        [ 1.6290, -0.5600, -1.4035],\n",
      "        [ 1.2554, -0.6550, -1.4452],\n",
      "        [ 1.6297, -0.5341, -1.5086],\n",
      "        [ 1.1436, -0.4346, -1.0911],\n",
      "        [ 1.4394, -0.4227, -1.2201],\n",
      "        [ 1.3283, -0.4731, -1.2791],\n",
      "        [ 1.4821, -0.5259, -1.2857],\n",
      "        [ 1.6431, -0.7638, -1.1084],\n",
      "        [ 1.0708, -0.6260, -1.2132],\n",
      "        [ 1.5560, -0.1949, -1.3519],\n",
      "        [ 1.5663, -0.7226, -1.2390],\n",
      "        [ 1.1820, -0.4429, -1.4251],\n",
      "        [ 1.4558, -0.5418, -1.0828],\n",
      "        [ 1.2915, -0.6167, -1.2794],\n",
      "        [ 1.2324, -0.5313, -1.3982],\n",
      "        [ 1.3678, -0.6565, -1.2172],\n",
      "        [ 1.3411, -0.6247, -0.9652],\n",
      "        [ 1.4257, -0.3612, -1.4186],\n",
      "        [ 1.2535, -0.2448, -1.2928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  13\n",
      "Batch:  (tensor([[  101,  1045,  2113,  ...,     0,     0,     0],\n",
      "        [  101,  2748,  1012,  ...,     0,     0,     0],\n",
      "        [  101,  1999,  2115,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2092,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  7632,  1012,  ...,     0,     0,     0],\n",
      "        [  101,  6316, 27522,  ...,     0,     0,     0]]), tensor([0, 0, 1, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.9417, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6160, -0.7913, -1.4370],\n",
      "        [ 1.9590, -0.5702, -1.6255],\n",
      "        [ 1.7439, -0.3518, -1.6274],\n",
      "        [ 1.7017, -0.7800, -1.4926],\n",
      "        [ 1.6002, -0.6534, -1.6396],\n",
      "        [ 1.8380, -0.8012, -1.6765],\n",
      "        [ 1.8118, -0.6433, -1.5489],\n",
      "        [ 1.9451, -0.7502, -1.6489],\n",
      "        [ 1.7168, -0.2745, -1.5260],\n",
      "        [ 1.4783, -0.5684, -1.7051],\n",
      "        [ 1.5204, -0.6747, -1.7159],\n",
      "        [ 1.8022, -0.6509, -1.5413],\n",
      "        [ 1.5624, -0.8800, -1.6081],\n",
      "        [ 1.8363, -0.6221, -1.5154],\n",
      "        [ 1.6209, -0.7123, -1.5440],\n",
      "        [ 1.9033, -0.4651, -1.4466],\n",
      "        [ 1.7878, -0.4766, -1.5975],\n",
      "        [ 1.6619, -0.7469, -1.5486],\n",
      "        [ 1.6973, -0.2294, -1.6002],\n",
      "        [ 1.7000, -0.3521, -1.4957],\n",
      "        [ 1.7131, -0.4599, -1.5377],\n",
      "        [ 1.4892, -0.7507, -1.6177],\n",
      "        [ 1.7045, -0.4899, -1.6075],\n",
      "        [ 1.7453, -0.6519, -1.5079],\n",
      "        [ 1.6461, -0.5634, -1.5139]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  14\n",
      "Batch:  (tensor([[ 101, 2748, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2903,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2145,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 3100, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 2672, 2074,  ...,    0,    0,    0],\n",
      "        [ 101, 2222, 6761,  ...,    0,    0,    0]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        2]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.4608, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8896, -0.5170, -1.5663],\n",
      "        [ 1.8320, -0.5320, -1.5165],\n",
      "        [ 1.9294, -0.3666, -1.3844],\n",
      "        [ 1.4793, -0.7067, -1.4256],\n",
      "        [ 1.8697, -0.6579, -1.5200],\n",
      "        [ 1.6751, -0.6784, -1.5573],\n",
      "        [ 1.6643, -0.9003, -1.5838],\n",
      "        [ 1.8847, -0.5495, -1.6679],\n",
      "        [ 2.0823, -0.3863, -1.9246],\n",
      "        [ 1.5803, -0.6408, -1.6700],\n",
      "        [ 1.8069, -1.0050, -1.2891],\n",
      "        [ 1.9543, -0.4838, -1.8071],\n",
      "        [ 1.8290, -0.3843, -1.5889],\n",
      "        [ 1.7286, -0.6370, -1.7015],\n",
      "        [ 1.8447, -0.5725, -1.5461],\n",
      "        [ 2.1208, -0.7885, -1.5931],\n",
      "        [ 1.5346, -0.6843, -1.6397],\n",
      "        [ 1.7381, -0.7620, -1.6714],\n",
      "        [ 1.8163, -0.5606, -1.7135],\n",
      "        [ 1.9787, -0.6641, -1.2736],\n",
      "        [ 1.5671, -0.8090, -1.5574],\n",
      "        [ 1.7472, -0.4118, -1.4870],\n",
      "        [ 1.6710, -0.7090, -1.4335],\n",
      "        [ 1.8831, -0.5653, -1.7687],\n",
      "        [ 1.8083, -0.7597, -1.6471]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  15\n",
      "Batch:  (tensor([[  101,  6316, 11684,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2123,  ...,     0,     0,     0],\n",
      "        [  101,  9389,  2072,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2057,  2064,  ...,     0,     0,     0],\n",
      "        [  101,  3398,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  8057,  1010,  ...,     0,     0,     0]]), tensor([2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.6045, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8397, -0.7153, -1.9708],\n",
      "        [ 1.6015, -0.6215, -1.4153],\n",
      "        [ 2.0121, -0.5772, -1.5639],\n",
      "        [ 2.0424, -0.7438, -1.7224],\n",
      "        [ 1.8148, -0.7495, -1.7734],\n",
      "        [ 2.1254, -0.6018, -1.6052],\n",
      "        [ 2.0747, -0.6484, -1.5660],\n",
      "        [ 1.8509, -0.6514, -1.6931],\n",
      "        [ 1.6921, -0.6179, -1.3141],\n",
      "        [ 1.7475, -0.6773, -1.8829],\n",
      "        [ 1.8323, -0.5809, -1.7017],\n",
      "        [ 1.8695, -0.7642, -1.3081],\n",
      "        [ 2.0589, -0.6347, -1.5149],\n",
      "        [ 2.0639, -0.4635, -1.7098],\n",
      "        [ 1.7371, -0.7506, -1.6282],\n",
      "        [ 1.9116, -0.7123, -1.6259],\n",
      "        [ 1.8878, -0.5961, -1.6163],\n",
      "        [ 2.0239, -0.5259, -1.4409],\n",
      "        [ 1.7765, -0.7456, -1.8553],\n",
      "        [ 1.8564, -0.4874, -1.5781],\n",
      "        [ 1.8240, -0.8372, -1.7441],\n",
      "        [ 1.8887, -0.5611, -1.8123],\n",
      "        [ 1.9517, -0.3019, -1.5244],\n",
      "        [ 1.9784, -0.6242, -1.8096],\n",
      "        [ 1.8607, -0.6030, -1.4718]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  16\n",
      "Batch:  (tensor([[  101,  1045,  2113,  ...,     0,     0,     0],\n",
      "        [  101,  1037,  2197,  ...,     0,     0,     0],\n",
      "        [  101, 18281,  1010,  ...,  1010,  2021,   102],\n",
      "        ...,\n",
      "        [  101,  2074,  2018,  ...,     0,     0,     0],\n",
      "        [  101,  7632,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  2498,  2008,  ...,     0,     0,     0]]), tensor([2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
      "        2]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.7857, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7309, -0.5401, -1.6839],\n",
      "        [ 1.7465, -0.3992, -1.3295],\n",
      "        [ 1.7771, -0.4910, -1.6528],\n",
      "        [ 1.7906, -0.3262, -1.9000],\n",
      "        [ 1.7184, -0.4543, -1.5926],\n",
      "        [ 1.4903, -0.5583, -1.7157],\n",
      "        [ 2.0101, -0.5898, -1.7451],\n",
      "        [ 1.7133, -0.6291, -1.5030],\n",
      "        [ 1.6695, -0.4698, -1.6258],\n",
      "        [ 1.7661, -0.5298, -1.5589],\n",
      "        [ 1.8987, -0.4579, -1.6113],\n",
      "        [ 1.8728, -0.6388, -1.4415],\n",
      "        [ 1.8483, -0.6835, -1.4397],\n",
      "        [ 1.6025, -0.6348, -1.6986],\n",
      "        [ 1.4788, -0.5825, -1.5343],\n",
      "        [ 1.9189, -0.7809, -1.7356],\n",
      "        [ 1.9430, -0.7182, -1.6476],\n",
      "        [ 1.6115, -0.8477, -1.4797],\n",
      "        [ 1.9663, -0.5957, -1.7996],\n",
      "        [ 1.7263, -0.7110, -1.5253],\n",
      "        [ 1.7908, -0.3866, -1.6928],\n",
      "        [ 1.7103, -0.5362, -1.6903],\n",
      "        [ 1.9090, -0.5849, -1.5938],\n",
      "        [ 1.5143, -0.4305, -1.6273],\n",
      "        [ 1.5381, -0.6955, -1.6191]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  17\n",
      "Batch:  (tensor([[ 101, 3100, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 2026, 3160,  ...,    0,    0,    0],\n",
      "        [ 101, 2004, 6287,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2957, 1037,  ...,    0,    0,    0],\n",
      "        [ 101, 2204, 2851,  ...,    0,    0,    0],\n",
      "        [ 101, 4931, 1010,  ...,    0,    0,    0]]), tensor([0, 0, 2, 0, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.8790, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5894, -0.8291, -1.1947],\n",
      "        [ 1.5297, -0.6726, -1.1655],\n",
      "        [ 1.4567, -0.7974, -1.4067],\n",
      "        [ 1.3561, -0.7789, -1.2378],\n",
      "        [ 1.5355, -0.7089, -1.3396],\n",
      "        [ 1.2892, -0.5168, -1.2447],\n",
      "        [ 1.4316, -0.7355, -1.1211],\n",
      "        [ 1.5155, -0.5667, -1.0176],\n",
      "        [ 1.2751, -0.8302, -1.1887],\n",
      "        [ 1.5076, -0.7758, -1.0334],\n",
      "        [ 1.5784, -0.8174, -1.2942],\n",
      "        [ 1.6034, -0.8949, -1.2794],\n",
      "        [ 1.3126, -0.8456, -0.9887],\n",
      "        [ 1.4983, -0.7012, -1.1395],\n",
      "        [ 1.4732, -0.7370, -1.1920],\n",
      "        [ 1.3687, -1.1369, -0.8142],\n",
      "        [ 1.6121, -0.6088, -1.3627],\n",
      "        [ 1.5777, -0.6418, -1.1983],\n",
      "        [ 1.5580, -0.7803, -1.2786],\n",
      "        [ 1.5117, -0.4766, -0.9128],\n",
      "        [ 1.4928, -0.6930, -1.5477],\n",
      "        [ 1.3834, -0.6918, -1.0661],\n",
      "        [ 1.3557, -0.8116, -1.1894],\n",
      "        [ 1.7728, -0.7110, -1.1743],\n",
      "        [ 1.4232, -0.6372, -1.1791]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  18\n",
      "Batch:  (tensor([[ 101, 1045, 2074,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2228,  ...,    0,    0,    0],\n",
      "        [ 101, 4067, 2017,  ..., 2655, 2247,  102],\n",
      "        ...,\n",
      "        [ 101, 3398, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2748, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 1996, 2034,  ...,    0,    0,    0]]), tensor([2, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.7581, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6594, -0.8634, -0.3810],\n",
      "        [ 0.9590, -0.8933, -0.6005],\n",
      "        [ 0.9146, -1.0393, -0.2338],\n",
      "        [ 0.8515, -0.9241, -0.1812],\n",
      "        [ 1.3462, -1.4053, -0.6069],\n",
      "        [ 0.8999, -0.8855, -0.2996],\n",
      "        [ 1.1018, -1.1564, -0.3192],\n",
      "        [ 1.2541, -1.0519, -0.4850],\n",
      "        [ 0.7121, -1.0592, -0.1993],\n",
      "        [ 0.9102, -0.8328, -0.4118],\n",
      "        [ 0.7915, -1.0234, -0.1198],\n",
      "        [ 1.1247, -0.9267, -0.4840],\n",
      "        [ 0.8024, -1.2128, -0.3404],\n",
      "        [ 1.1160, -0.9312, -0.4201],\n",
      "        [ 1.0562, -1.0116, -0.1769],\n",
      "        [ 0.9738, -0.9663, -0.3260],\n",
      "        [ 0.9329, -1.2618, -0.2168],\n",
      "        [ 1.1807, -1.1648, -0.4802],\n",
      "        [ 0.9096, -0.9491, -0.4577],\n",
      "        [ 0.8873, -1.2016, -0.2304],\n",
      "        [ 1.0663, -1.3254, -0.4209],\n",
      "        [ 1.0170, -0.8805, -0.0887],\n",
      "        [ 1.2499, -1.0534, -0.6445],\n",
      "        [ 0.7945, -1.1347, -0.1031],\n",
      "        [ 0.6438, -1.1010, -0.3579]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  19\n",
      "Batch:  (tensor([[ 101, 2307, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 4283, 2005,  ...,    0,    0,    0],\n",
      "        [ 101, 4067, 2017,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 4283, 2200,  ...,    0,    0,    0],\n",
      "        [ 101, 2307, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 3398, 1012,  ...,    0,    0,    0]]), tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.8481, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3108, -1.4101,  0.3474],\n",
      "        [ 0.7000, -1.2291,  0.4800],\n",
      "        [ 0.8043, -1.3615,  0.5854],\n",
      "        [ 0.5850, -1.2739,  0.5456],\n",
      "        [ 0.2372, -1.4645,  0.7342],\n",
      "        [ 0.4814, -1.4672,  0.4520],\n",
      "        [ 0.4042, -1.1805,  0.4722],\n",
      "        [ 0.5837, -1.2530,  0.7889],\n",
      "        [ 0.6708, -1.5483,  0.2689],\n",
      "        [ 0.5398, -1.3123,  0.6385],\n",
      "        [ 0.4688, -1.3372,  0.4625],\n",
      "        [ 0.5426, -1.3944,  0.5026],\n",
      "        [ 0.4775, -1.1836,  0.4703],\n",
      "        [ 0.4965, -1.2728,  0.3657],\n",
      "        [ 0.2952, -1.5705,  0.7478],\n",
      "        [ 0.4831, -1.1358,  0.2564],\n",
      "        [ 0.4148, -1.3206,  0.2908],\n",
      "        [ 0.4391, -1.3280,  0.4417],\n",
      "        [ 0.3905, -1.2870,  0.3649],\n",
      "        [ 0.7712, -1.4557,  0.2986],\n",
      "        [ 0.7279, -1.3124,  0.4928],\n",
      "        [ 0.3114, -1.2394,  0.3665],\n",
      "        [ 0.3326, -1.4202,  0.5148],\n",
      "        [ 0.6455, -1.3369,  0.5819],\n",
      "        [ 0.4533, -1.1970,  0.5061]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  20\n",
      "Batch:  (tensor([[  101,  2061,  1045,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2052,  ...,     0,     0,     0],\n",
      "        [  101,  2057,  1521,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  5074, 11409,  ...,     0,     0,     0],\n",
      "        [  101,  4931,  4364,  ...,     0,     0,     0],\n",
      "        [  101,  2061,  2003,  ...,     0,     0,     0]]), tensor([0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.4760, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8172, -1.7379,  0.3262],\n",
      "        [ 1.0879, -1.9714,  0.1763],\n",
      "        [ 1.1388, -1.8295,  0.0491],\n",
      "        [ 1.2457, -1.7136,  0.2746],\n",
      "        [ 1.3067, -1.6147, -0.2161],\n",
      "        [ 1.1911, -1.7196,  0.0594],\n",
      "        [ 1.1658, -1.5134, -0.0659],\n",
      "        [ 1.0998, -1.6718,  0.2008],\n",
      "        [ 1.2329, -1.6087,  0.3406],\n",
      "        [ 1.1650, -1.6660,  0.1669],\n",
      "        [ 1.3275, -1.5983,  0.1475],\n",
      "        [ 1.2271, -1.7843,  0.1709],\n",
      "        [ 1.0821, -1.7189,  0.4918],\n",
      "        [ 0.9865, -1.5360,  0.1029],\n",
      "        [ 0.9021, -1.9338,  0.3513],\n",
      "        [ 0.9269, -1.3226,  0.3147],\n",
      "        [ 1.1157, -1.8544, -0.0394],\n",
      "        [ 1.1623, -1.8414, -0.0530],\n",
      "        [ 1.2730, -1.8046,  0.2425],\n",
      "        [ 1.4118, -1.4967, -0.1075],\n",
      "        [ 1.0418, -1.5758,  0.0993],\n",
      "        [ 1.1608, -1.9039,  0.3622],\n",
      "        [ 1.0234, -1.6508,  0.5593],\n",
      "        [ 0.8326, -1.8814,  0.3932],\n",
      "        [ 1.3723, -2.0944,  0.1954]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  21\n",
      "Batch:  (tensor([[ 101, 2054, 1045,  ...,    0,    0,    0],\n",
      "        [ 101, 4283, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2763, 2087,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2009, 1521,  ...,    0,    0,    0],\n",
      "        [ 101, 3100, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2157, 1010,  ...,    0,    0,    0]]), tensor([1, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.9451, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.0770, -1.9858, -0.5422],\n",
      "        [ 1.8107, -1.6815, -0.2306],\n",
      "        [ 1.9012, -2.1632, -0.2447],\n",
      "        [ 1.9018, -2.1603, -0.5258],\n",
      "        [ 1.8717, -2.1841, -0.2929],\n",
      "        [ 1.8895, -2.1589, -0.2751],\n",
      "        [ 1.9121, -2.1758, -0.2336],\n",
      "        [ 1.7824, -2.2301, -0.2043],\n",
      "        [ 2.0107, -2.1892, -0.2139],\n",
      "        [ 1.7710, -2.1623, -0.2833],\n",
      "        [ 1.4688, -2.0572, -0.2517],\n",
      "        [ 1.8575, -2.2143, -0.2577],\n",
      "        [ 1.9676, -1.9953, -0.4023],\n",
      "        [ 1.7685, -2.1591, -0.2515],\n",
      "        [ 1.7677, -2.0885, -0.3570],\n",
      "        [ 1.7742, -1.9746, -0.3087],\n",
      "        [ 1.8125, -1.9611,  0.0059],\n",
      "        [ 1.8206, -1.9337, -0.2493],\n",
      "        [ 1.9395, -2.1820, -0.2258],\n",
      "        [ 2.0503, -1.9672, -0.0804],\n",
      "        [ 1.9392, -2.1252, -0.3418],\n",
      "        [ 1.7512, -2.2237, -0.2755],\n",
      "        [ 1.9298, -2.0223, -0.1132],\n",
      "        [ 1.7486, -1.7601, -0.2639],\n",
      "        [ 1.8278, -2.3213, -0.1608]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  22\n",
      "Batch:  (tensor([[  101,  4717, 10588,  ...,     0,     0,     0],\n",
      "        [  101,  7632,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  4283,  2005,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2092,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  4283,  8057,  ...,     0,     0,     0],\n",
      "        [  101,  1998,  1996,  ...,     0,     0,     0]]), tensor([0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0,\n",
      "        2]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(1.1054, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1319, -2.2261, -0.8115],\n",
      "        [ 1.9030, -2.1230, -0.7341],\n",
      "        [ 2.3326, -1.9298, -0.6794],\n",
      "        [ 1.8412, -2.0264, -0.8336],\n",
      "        [ 2.0588, -2.0467, -0.8452],\n",
      "        [ 2.0160, -1.8270, -0.5114],\n",
      "        [ 2.0197, -2.0087, -0.7143],\n",
      "        [ 1.8374, -2.4086, -0.4673],\n",
      "        [ 1.9699, -2.2965, -0.7624],\n",
      "        [ 2.0316, -2.0158, -0.5545],\n",
      "        [ 2.1021, -1.6662, -0.6469],\n",
      "        [ 1.9538, -2.0276, -0.6388],\n",
      "        [ 1.9099, -1.9770, -0.8162],\n",
      "        [ 2.0357, -1.8411, -0.5844],\n",
      "        [ 2.1182, -1.5471, -0.8185],\n",
      "        [ 2.1953, -1.7537, -0.4758],\n",
      "        [ 2.0186, -2.2634, -0.3524],\n",
      "        [ 2.2131, -2.1639, -0.5986],\n",
      "        [ 2.1713, -1.9949, -0.5260],\n",
      "        [ 2.0468, -2.1630, -0.5249],\n",
      "        [ 2.1986, -1.8042, -0.3593],\n",
      "        [ 1.9700, -1.8684, -0.4039],\n",
      "        [ 2.1355, -2.1283, -0.5429],\n",
      "        [ 1.9092, -2.2020, -0.6348],\n",
      "        [ 1.9485, -1.9070, -0.6955]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  23\n",
      "Batch:  (tensor([[  101,  2008,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  4067,  2017,  ...,     0,     0,     0],\n",
      "        [  101,  2021,  2017,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2054,  3867,  ...,     0,     0,     0],\n",
      "        [  101,  1998, 16378,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0]]), tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(1.0783, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8474, -1.7527, -0.7217],\n",
      "        [ 1.7846, -1.9996, -0.8052],\n",
      "        [ 1.8247, -2.1030, -0.6377],\n",
      "        [ 2.0016, -2.0204, -0.7552],\n",
      "        [ 1.6876, -2.0466, -0.3772],\n",
      "        [ 1.7637, -1.9919, -0.5329],\n",
      "        [ 2.0417, -1.5785, -0.4100],\n",
      "        [ 1.8375, -2.0894, -0.4622],\n",
      "        [ 1.8812, -2.2218, -0.4343],\n",
      "        [ 1.7689, -2.2576, -0.4403],\n",
      "        [ 1.5583, -2.0343, -0.1952],\n",
      "        [ 2.0719, -2.0086, -0.6250],\n",
      "        [ 1.9170, -1.9544, -0.5311],\n",
      "        [ 1.9309, -2.1310, -0.4292],\n",
      "        [ 1.4303, -2.0545, -0.3240],\n",
      "        [ 1.9117, -1.8479, -0.7111],\n",
      "        [ 2.2067, -1.9385, -0.4981],\n",
      "        [ 2.1509, -1.6728, -0.5000],\n",
      "        [ 1.8677, -1.8959, -0.6644],\n",
      "        [ 1.8939, -1.5968, -0.4601],\n",
      "        [ 1.7231, -1.9805, -0.5875],\n",
      "        [ 1.7106, -1.8919, -0.4347],\n",
      "        [ 1.7076, -1.8168, -0.5616],\n",
      "        [ 1.6939, -2.0435, -0.5947],\n",
      "        [ 2.2267, -2.2294, -0.2822]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  24\n",
      "Batch:  (tensor([[ 101, 3100, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 2053, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 2053, 1012,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 4067, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 4923, 2003,  ...,    0,    0,    0],\n",
      "        [ 101, 5525, 1010,  ...,    0,    0,    0]]), tensor([0, 1, 0, 2, 2, 2, 2, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0,\n",
      "        2]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(1.1418, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1078, -1.4530, -0.1924],\n",
      "        [ 1.4677, -1.6622, -0.6339],\n",
      "        [ 1.1131, -1.6637,  0.0319],\n",
      "        [ 1.6398, -1.4521, -0.2089],\n",
      "        [ 1.3819, -1.3984, -0.2812],\n",
      "        [ 1.6433, -1.3612, -0.1983],\n",
      "        [ 1.3914, -1.5095,  0.0594],\n",
      "        [ 1.4184, -1.5158, -0.1342],\n",
      "        [ 1.3690, -1.9121, -0.3418],\n",
      "        [ 1.3570, -1.2771, -0.4262],\n",
      "        [ 1.5473, -1.6351, -0.4609],\n",
      "        [ 1.6551, -1.8243, -0.3279],\n",
      "        [ 1.5403, -1.6382, -0.1972],\n",
      "        [ 1.6387, -1.3715, -0.3181],\n",
      "        [ 1.3399, -1.4467, -0.4607],\n",
      "        [ 1.4116, -1.5604, -0.2915],\n",
      "        [ 1.5798, -1.4765, -0.1722],\n",
      "        [ 1.6269, -1.6629, -0.1313],\n",
      "        [ 1.4347, -1.9407, -0.3044],\n",
      "        [ 1.6436, -1.5542, -0.3231],\n",
      "        [ 1.4610, -1.3605, -0.0856],\n",
      "        [ 1.6891, -1.7364, -0.1647],\n",
      "        [ 1.5915, -1.6316, -0.5342],\n",
      "        [ 1.7361, -1.5327, -0.4915],\n",
      "        [ 1.5133, -1.5769, -0.0522]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  25\n",
      "Batch:  (tensor([[ 101, 1045, 2074,  ...,    0,    0,    0],\n",
      "        [ 101, 2198, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 4283, 1010,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2307, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2157, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 4067, 2017,  ...,    0,    0,    0]]), tensor([0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.6170, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9142, -1.2652, -0.2598],\n",
      "        [ 0.9433, -1.3978,  0.2963],\n",
      "        [ 0.9737, -1.2448,  0.1050],\n",
      "        [ 0.9018, -1.3413,  0.1185],\n",
      "        [ 0.9587, -1.2702, -0.1265],\n",
      "        [ 1.1439, -1.3749, -0.0687],\n",
      "        [ 1.0128, -1.3973,  0.0648],\n",
      "        [ 1.1018, -1.1690,  0.0126],\n",
      "        [ 0.8977, -1.0818,  0.0756],\n",
      "        [ 0.6515, -1.1686,  0.0639],\n",
      "        [ 0.6518, -1.0315,  0.0906],\n",
      "        [ 0.8785, -1.5788,  0.1153],\n",
      "        [ 1.0017, -1.0575,  0.4133],\n",
      "        [ 1.0528, -1.1392,  0.1445],\n",
      "        [ 0.8955, -1.2486,  0.1929],\n",
      "        [ 0.9159, -1.4768, -0.2577],\n",
      "        [ 0.8697, -1.3040,  0.0881],\n",
      "        [ 0.7838, -1.4789,  0.2262],\n",
      "        [ 1.1492, -1.3615, -0.1955],\n",
      "        [ 0.8940, -1.2862, -0.0869],\n",
      "        [ 0.8219, -1.0377,  0.1369],\n",
      "        [ 0.6976, -1.3440,  0.0827],\n",
      "        [ 0.9008, -1.1630,  0.1893],\n",
      "        [ 1.2828, -1.1432, -0.1382],\n",
      "        [ 0.7881, -1.1506,  0.0478]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  26\n",
      "Batch:  (tensor([[ 101, 2011, 1996,  ...,    0,    0,    0],\n",
      "        [ 101, 3100, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2092, 1010,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2061, 1045,  ...,    0,    0,    0],\n",
      "        [ 101, 2129, 2024,  ...,    0,    0,    0],\n",
      "        [ 101, 4283, 1010,  ...,    0,    0,    0]]), tensor([0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.6791, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7194, -0.9731,  0.1591],\n",
      "        [ 0.8272, -1.1431, -0.0063],\n",
      "        [ 0.5054, -0.8038,  0.1385],\n",
      "        [ 0.9292, -0.8632, -0.0788],\n",
      "        [ 0.6973, -0.8938,  0.2724],\n",
      "        [ 0.6929, -0.8508, -0.0861],\n",
      "        [ 0.4214, -1.1684,  0.0809],\n",
      "        [ 0.4677, -1.0813,  0.5416],\n",
      "        [ 0.8434, -1.0525,  0.2506],\n",
      "        [ 0.7353, -1.2745,  0.3058],\n",
      "        [ 0.5025, -1.0661, -0.0937],\n",
      "        [ 0.8426, -0.8221,  0.6244],\n",
      "        [ 0.5260, -1.0128,  0.2567],\n",
      "        [ 0.2363, -1.0246,  0.1013],\n",
      "        [ 0.7845, -0.9676,  0.3033],\n",
      "        [ 0.7765, -1.1636, -0.0366],\n",
      "        [ 0.6470, -1.1207,  0.3090],\n",
      "        [ 0.6590, -0.9279,  0.3904],\n",
      "        [ 0.8705, -0.9390,  0.3965],\n",
      "        [ 0.8315, -0.9847,  0.0795],\n",
      "        [ 0.7479, -1.1694,  0.2885],\n",
      "        [ 0.7483, -0.7502, -0.2494],\n",
      "        [ 0.8641, -0.8858,  0.1680],\n",
      "        [ 0.7303, -0.9307,  0.0783],\n",
      "        [ 0.5763, -0.8999,  0.2919]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  27\n",
      "Batch:  (tensor([[ 101, 3505, 2003,  ...,    0,    0,    0],\n",
      "        [ 101, 2061, 2048,  ...,    0,    0,    0],\n",
      "        [ 101, 3100, 1010,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1045, 1005,  ...,    0,    0,    0],\n",
      "        [ 101, 2959, 4284,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2074,  ...,    0,    0,    0]]), tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.6821, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6926, -0.9931,  0.0697],\n",
      "        [ 0.7793, -1.0711,  0.4340],\n",
      "        [ 0.6697, -1.0288,  0.2209],\n",
      "        [ 0.6048, -1.2550,  0.1084],\n",
      "        [ 0.8930, -0.9077,  0.0492],\n",
      "        [ 0.4794, -0.9340,  0.1828],\n",
      "        [ 0.3794, -1.1953,  0.2032],\n",
      "        [ 0.6211, -0.9502,  0.2605],\n",
      "        [ 0.9561, -1.0806,  0.1762],\n",
      "        [ 0.5632, -0.7360,  0.1227],\n",
      "        [ 0.5285, -0.6632,  0.3410],\n",
      "        [ 0.7425, -1.0400,  0.1346],\n",
      "        [ 0.6184, -0.8543, -0.0236],\n",
      "        [ 0.7100, -0.9493,  0.1932],\n",
      "        [ 0.7166, -1.1925,  0.0100],\n",
      "        [ 0.5949, -0.9229,  0.4257],\n",
      "        [ 0.5969, -1.0860,  0.1314],\n",
      "        [ 0.7195, -1.2702,  0.2577],\n",
      "        [ 0.5282, -0.8416,  0.0674],\n",
      "        [ 0.8723, -0.7060,  0.1771],\n",
      "        [ 0.5611, -1.0011,  0.4907],\n",
      "        [ 0.8098, -1.0208, -0.0678],\n",
      "        [ 0.7606, -1.4025,  0.2462],\n",
      "        [ 0.4358, -1.0425,  0.1593],\n",
      "        [ 0.5585, -1.1180,  0.2092]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  28\n",
      "Batch:  (tensor([[ 101, 2057, 1521,  ...,    0,    0,    0],\n",
      "        [ 101, 2469, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 4067, 2017,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2061, 2928,  ...,    0,    0,    0],\n",
      "        [ 101, 2748, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 2061, 1010,  ...,    0,    0,    0]]), tensor([0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.7169, grad_fn=<NllLossBackward0>), logits=tensor([[ 8.5776e-01, -9.4370e-01,  2.1025e-01],\n",
      "        [ 6.0784e-01, -1.0393e+00, -1.9395e-02],\n",
      "        [ 6.9695e-01, -1.1321e+00, -4.1623e-03],\n",
      "        [ 1.0561e+00, -1.0909e+00,  7.4445e-02],\n",
      "        [ 8.8967e-01, -9.6341e-01, -2.1376e-02],\n",
      "        [ 8.4415e-01, -1.1345e+00,  8.9165e-02],\n",
      "        [ 9.6137e-01, -8.8246e-01,  9.2749e-02],\n",
      "        [ 7.3310e-01, -1.3225e+00,  1.2680e-01],\n",
      "        [ 8.5860e-01, -8.7279e-01,  1.6621e-01],\n",
      "        [ 1.0140e+00, -1.2605e+00,  1.4006e-02],\n",
      "        [ 7.4869e-01, -7.8023e-01,  1.4305e-01],\n",
      "        [ 8.6883e-01, -9.5902e-01,  1.0864e-01],\n",
      "        [ 8.7807e-01, -7.8808e-01,  1.1840e-01],\n",
      "        [ 7.6977e-01, -9.0150e-01,  3.4379e-03],\n",
      "        [ 6.9726e-01, -1.2002e+00, -1.2616e-03],\n",
      "        [ 8.8979e-01, -1.2644e+00,  1.7964e-01],\n",
      "        [ 8.7128e-01, -9.3082e-01, -9.3367e-03],\n",
      "        [ 9.0906e-01, -1.2795e+00,  1.1604e-01],\n",
      "        [ 7.2666e-01, -1.1357e+00, -2.7072e-01],\n",
      "        [ 7.6083e-01, -1.2417e+00, -1.2705e-01],\n",
      "        [ 6.6767e-01, -1.0360e+00, -1.0120e-01],\n",
      "        [ 5.3927e-01, -1.1199e+00, -1.4724e-02],\n",
      "        [ 8.5773e-01, -1.0173e+00, -7.4575e-03],\n",
      "        [ 8.3986e-01, -1.2524e+00, -5.5601e-02],\n",
      "        [ 8.6778e-01, -1.0748e+00,  7.6416e-02]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  29\n",
      "Batch:  (tensor([[  101,  2469,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  4067,  2017,  ...,     0,     0,     0],\n",
      "        [  101,  2748,  9120,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  4459, 19610,  ...,     0,     0,     0],\n",
      "        [  101,  2054,  1045,  ...,     0,     0,     0],\n",
      "        [  101,  1998,  2059,  ...,     0,     0,     0]]), tensor([1, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 2, 0, 2,\n",
      "        2]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.9272, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0379, -1.2163, -0.2942],\n",
      "        [ 1.1872, -1.3209, -0.1693],\n",
      "        [ 1.0429, -1.1583, -0.3211],\n",
      "        [ 1.0294, -1.0845, -0.4435],\n",
      "        [ 1.0804, -1.0663, -0.2022],\n",
      "        [ 1.4550, -1.5143, -0.1208],\n",
      "        [ 1.2518, -1.3285, -0.3640],\n",
      "        [ 1.1452, -1.2190,  0.0553],\n",
      "        [ 1.3322, -1.2397, -0.0122],\n",
      "        [ 1.1758, -0.9115, -0.5571],\n",
      "        [ 0.9969, -1.0483, -0.3504],\n",
      "        [ 1.2822, -0.7621, -0.2220],\n",
      "        [ 1.0674, -1.2194, -0.4103],\n",
      "        [ 1.1602, -1.3173, -0.2686],\n",
      "        [ 1.0816, -1.2089, -0.6289],\n",
      "        [ 1.0151, -0.9056, -0.0922],\n",
      "        [ 1.2569, -1.0988, -0.3158],\n",
      "        [ 1.2452, -1.0386, -0.1093],\n",
      "        [ 1.3377, -1.2602, -0.3297],\n",
      "        [ 1.0455, -1.2428, -0.5712],\n",
      "        [ 0.9754, -0.9359, -0.0955],\n",
      "        [ 1.3481, -0.7031, -0.3967],\n",
      "        [ 1.1455, -1.2444, -0.3182],\n",
      "        [ 1.1874, -1.0977, -0.2154],\n",
      "        [ 1.1302, -1.1306, -0.0650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  30\n",
      "Batch:  (tensor([[ 101, 2006, 3931,  ...,    0,    0,    0],\n",
      "        [ 101, 4067, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 3398, 1012,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2748, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 4067, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 3100, 1012,  ...,    0,    0,    0]]), tensor([1, 0, 1, 2, 2, 0, 1, 2, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 2,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(1.1298, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1251, -1.4189, -0.3734],\n",
      "        [ 1.4080, -1.3524, -0.3315],\n",
      "        [ 1.5273, -1.2190, -0.5964],\n",
      "        [ 0.9278, -1.3818, -0.5931],\n",
      "        [ 1.1994, -1.4319, -0.1255],\n",
      "        [ 1.6293, -1.2226, -0.5478],\n",
      "        [ 0.9932, -1.0506, -0.4571],\n",
      "        [ 1.2233, -1.2478, -0.3882],\n",
      "        [ 1.3115, -0.9736, -0.2915],\n",
      "        [ 1.4334, -1.4599, -0.3947],\n",
      "        [ 1.2624, -1.2981, -0.5126],\n",
      "        [ 1.1512, -1.1868, -0.3761],\n",
      "        [ 1.1461, -1.2377, -0.6665],\n",
      "        [ 1.5828, -1.1517, -0.4696],\n",
      "        [ 1.3645, -1.3522, -0.4590],\n",
      "        [ 1.1526, -0.7828, -0.5042],\n",
      "        [ 1.2842, -0.9543, -0.3695],\n",
      "        [ 1.1625, -1.2301, -0.7592],\n",
      "        [ 1.2425, -1.1812, -0.6549],\n",
      "        [ 1.1877, -1.2732, -0.4583],\n",
      "        [ 1.0768, -1.2345, -0.0502],\n",
      "        [ 1.2128, -1.0760, -0.3419],\n",
      "        [ 1.3600, -1.0498, -0.5198],\n",
      "        [ 1.2790, -1.3774, -0.4151],\n",
      "        [ 1.3633, -1.1707, -0.6425]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  31\n",
      "Batch:  (tensor([[ 101, 2204, 2851,  ...,    0,    0,    0],\n",
      "        [ 101, 7525, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2307, 1012,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 4931, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 7632, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 4283, 1010,  ...,    0,    0,    0]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.3855, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.5086, -1.1792, -0.3871],\n",
      "        [ 1.3628, -1.1861, -0.6383],\n",
      "        [ 1.4351, -1.1685, -0.1591],\n",
      "        [ 1.5768, -0.9538, -0.5114],\n",
      "        [ 1.4082, -0.9982, -0.5287],\n",
      "        [ 1.4578, -1.1653, -0.4410],\n",
      "        [ 1.3344, -1.2233, -0.2865],\n",
      "        [ 1.3350, -1.2314, -0.4020],\n",
      "        [ 1.1609, -1.0067, -0.6363],\n",
      "        [ 1.2599, -0.9587, -0.3611],\n",
      "        [ 1.1749, -0.9053, -0.6206],\n",
      "        [ 1.1966, -1.2757, -0.3900],\n",
      "        [ 1.4200, -0.8656, -0.5150],\n",
      "        [ 1.0289, -1.4732, -0.5417],\n",
      "        [ 1.1963, -1.2620, -0.9658],\n",
      "        [ 1.2983, -1.2136, -0.4166],\n",
      "        [ 1.3647, -1.0067, -0.6813],\n",
      "        [ 1.2917, -1.2342, -0.6249],\n",
      "        [ 1.4145, -0.9970, -0.8418],\n",
      "        [ 1.3170, -1.1665, -0.5049],\n",
      "        [ 1.4093, -1.1092, -0.6897],\n",
      "        [ 1.1404, -1.1196, -0.2821],\n",
      "        [ 0.9582, -1.1607, -0.5945],\n",
      "        [ 1.2046, -1.2475, -0.4061],\n",
      "        [ 1.3920, -1.0726, -0.4935]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  32\n",
      "Batch:  (tensor([[  101,  4067,  2017,  ...,     0,     0,     0],\n",
      "        [  101, 22267,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  2288,  2009,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1045,  2052,  ...,     0,     0,     0],\n",
      "        [  101,  4067,  2017,  ...,     0,     0,     0],\n",
      "        [  101,  1998,  1010,  ...,     0,     0,     0]]), tensor([0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.6545, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2389, -0.8360, -0.6923],\n",
      "        [ 1.1288, -0.9765, -0.7620],\n",
      "        [ 1.3660, -1.2039, -0.5615],\n",
      "        [ 1.3728, -0.8822, -0.4838],\n",
      "        [ 1.1865, -1.2132, -0.6522],\n",
      "        [ 1.1911, -1.2496, -0.7068],\n",
      "        [ 1.3133, -1.1246, -0.3061],\n",
      "        [ 1.5522, -1.0371, -0.3891],\n",
      "        [ 1.3480, -1.2568, -0.5552],\n",
      "        [ 1.3987, -0.8311, -0.7602],\n",
      "        [ 1.4039, -0.8564, -0.8793],\n",
      "        [ 1.3338, -0.9879, -0.3648],\n",
      "        [ 1.3026, -1.1155, -0.6732],\n",
      "        [ 1.2552, -1.1833, -0.7034],\n",
      "        [ 1.2055, -0.6661, -0.6835],\n",
      "        [ 1.4607, -1.0597, -0.6119],\n",
      "        [ 1.1343, -0.7304, -0.6763],\n",
      "        [ 1.2429, -1.1407, -0.8780],\n",
      "        [ 1.4293, -0.8421, -0.8065],\n",
      "        [ 1.4784, -1.0814, -0.4802],\n",
      "        [ 1.1772, -0.8689, -0.8685],\n",
      "        [ 1.4621, -1.1766, -0.6850],\n",
      "        [ 1.2113, -0.8949, -0.8592],\n",
      "        [ 1.4015, -1.2410, -0.7083],\n",
      "        [ 1.1309, -1.0727, -0.5676]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  33\n",
      "Batch:  (tensor([[ 101, 4283, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2092, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 2092, 1010,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2307, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 1998, 2079,  ...,    0,    0,    0],\n",
      "        [ 101, 2204, 5027,  ...,    0,    0,    0]]), tensor([0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        2]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.6234, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3235, -0.9326, -0.7140],\n",
      "        [ 1.2383, -1.1672, -0.9009],\n",
      "        [ 1.1902, -1.0691, -0.6874],\n",
      "        [ 1.3481, -1.2796, -0.8323],\n",
      "        [ 1.3124, -1.0231, -0.7864],\n",
      "        [ 1.4961, -1.0345, -1.0183],\n",
      "        [ 1.1246, -1.0877, -0.6347],\n",
      "        [ 1.3174, -1.0294, -0.9989],\n",
      "        [ 1.3592, -1.0847, -0.8944],\n",
      "        [ 1.4071, -0.7827, -0.7306],\n",
      "        [ 1.2602, -1.1225, -0.9799],\n",
      "        [ 1.1874, -0.7932, -0.7455],\n",
      "        [ 1.4968, -0.4181, -0.7510],\n",
      "        [ 1.5796, -1.0942, -0.8312],\n",
      "        [ 1.3616, -1.1923, -0.7061],\n",
      "        [ 1.1015, -0.9923, -0.5444],\n",
      "        [ 1.5997, -0.9258, -0.7700],\n",
      "        [ 1.3007, -0.8922, -0.3653],\n",
      "        [ 1.4745, -0.8918, -0.9263],\n",
      "        [ 1.2239, -0.7698, -0.6265],\n",
      "        [ 1.5570, -0.5630, -0.5553],\n",
      "        [ 1.4642, -0.6891, -0.6804],\n",
      "        [ 1.3702, -0.8647, -0.9388],\n",
      "        [ 1.0101, -1.0266, -0.7187],\n",
      "        [ 1.3608, -0.7159, -0.7762]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  34\n",
      "Batch:  (tensor([[  101,  4283,  1010,  ...,     0,     0,     0],\n",
      "        [  101, 10303,  1012,  ...,     0,     0,     0],\n",
      "        [  101,  7078,  1012,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  4283,  1010,  ...,     0,     0,     0],\n",
      "        [  101, 16222, 13465,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  4341,  ...,     0,     0,     0]]), tensor([0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0,\n",
      "        1]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.9232, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4779, -0.8919, -0.7566],\n",
      "        [ 1.1982, -0.8628, -0.9857],\n",
      "        [ 1.5406, -0.9891, -0.5782],\n",
      "        [ 1.5512, -0.8025, -0.6857],\n",
      "        [ 1.3431, -0.9587, -0.9754],\n",
      "        [ 1.4271, -1.1650, -0.5953],\n",
      "        [ 1.4067, -0.9004, -0.7350],\n",
      "        [ 1.3872, -0.8229, -1.0473],\n",
      "        [ 1.3649, -0.9352, -0.7570],\n",
      "        [ 1.4346, -0.7693, -0.6772],\n",
      "        [ 1.4655, -0.7990, -0.8183],\n",
      "        [ 1.5430, -0.8491, -0.6929],\n",
      "        [ 1.5406, -0.6442, -0.9848],\n",
      "        [ 1.4881, -1.0601, -0.7420],\n",
      "        [ 1.2322, -1.0525, -0.7496],\n",
      "        [ 1.2619, -0.8771, -0.9745],\n",
      "        [ 1.3416, -0.9114, -0.8611],\n",
      "        [ 1.3410, -0.7888, -0.9526],\n",
      "        [ 1.2582, -0.8284, -1.1449],\n",
      "        [ 1.4903, -0.7565, -0.8538],\n",
      "        [ 1.5437, -0.8276, -0.9090],\n",
      "        [ 1.4333, -0.8889, -0.7093],\n",
      "        [ 1.4470, -0.7648, -1.0493],\n",
      "        [ 1.6467, -0.6240, -0.9943],\n",
      "        [ 1.4361, -0.9964, -0.7013]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  35\n",
      "Batch:  (tensor([[ 101, 1045, 2228,  ...,    0,    0,    0],\n",
      "        [ 101, 2087, 1997,  ...,    0,    0,    0],\n",
      "        [ 101, 2288, 2009,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2092, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 8016, 2033,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2228,  ...,    0,    0,    0]]), tensor([2, 2, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(1.0557, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4950, -0.6567, -0.9756],\n",
      "        [ 1.4201, -0.8580, -0.6250],\n",
      "        [ 1.4231, -0.6611, -1.1075],\n",
      "        [ 1.5535, -0.8159, -0.7804],\n",
      "        [ 1.2311, -0.6872, -0.7070],\n",
      "        [ 1.2597, -0.7512, -0.9768],\n",
      "        [ 1.4143, -1.0112, -0.6319],\n",
      "        [ 1.4945, -0.7557, -0.6960],\n",
      "        [ 1.2570, -0.8546, -1.0824],\n",
      "        [ 1.3639, -0.7764, -0.8423],\n",
      "        [ 1.3952, -0.8792, -1.0692],\n",
      "        [ 1.2453, -0.6614, -0.8897],\n",
      "        [ 1.2537, -0.7206, -0.5653],\n",
      "        [ 1.3384, -0.8358, -0.7980],\n",
      "        [ 1.4248, -1.0065, -1.0114],\n",
      "        [ 1.1822, -0.8857, -0.5495],\n",
      "        [ 1.3387, -0.9417, -0.7607],\n",
      "        [ 1.6608, -0.7009, -0.4940],\n",
      "        [ 1.4054, -0.5352, -0.9696],\n",
      "        [ 1.2849, -0.7607, -0.8806],\n",
      "        [ 1.3113, -0.6453, -0.6897],\n",
      "        [ 1.2072, -0.6076, -0.7820],\n",
      "        [ 1.0091, -0.9807, -0.5734],\n",
      "        [ 1.3609, -0.9417, -0.8865],\n",
      "        [ 1.3814, -0.8819, -0.9305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  36\n",
      "Batch:  (tensor([[  101,  2307,  1012,  ...,     0,     0,     0],\n",
      "        [  101,  2007,  4847,  ...,     0,     0,     0],\n",
      "        [  101,  2017,  1521,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2065,  2057,  ...,     0,     0,     0],\n",
      "        [  101, 10881,  1055,  ...,     0,     0,     0],\n",
      "        [  101,  2307,  1010,  ...,     0,     0,     0]]), tensor([0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.8578, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1338, -0.8801, -0.3177],\n",
      "        [ 1.3478, -0.8587, -0.2153],\n",
      "        [ 1.3001, -1.0714, -0.6365],\n",
      "        [ 1.1761, -0.8671, -0.9234],\n",
      "        [ 1.2172, -0.8145, -0.6817],\n",
      "        [ 1.2782, -0.9176, -0.6080],\n",
      "        [ 1.2752, -0.9246, -0.4080],\n",
      "        [ 1.1457, -0.9702, -0.5112],\n",
      "        [ 1.0650, -0.6692, -0.8311],\n",
      "        [ 1.1472, -0.9284, -0.5489],\n",
      "        [ 1.1887, -1.0396, -0.4271],\n",
      "        [ 1.4078, -0.9224, -0.6492],\n",
      "        [ 1.1642, -1.2294, -0.6728],\n",
      "        [ 1.2493, -1.2134, -0.6421],\n",
      "        [ 0.9672, -0.7997, -0.6098],\n",
      "        [ 1.3193, -1.0344, -0.6903],\n",
      "        [ 1.2337, -0.8792, -0.3650],\n",
      "        [ 0.9703, -0.9336, -0.4829],\n",
      "        [ 1.2816, -0.7504, -0.7104],\n",
      "        [ 1.1367, -0.9060, -0.3491],\n",
      "        [ 0.9646, -0.9765, -0.4409],\n",
      "        [ 1.2132, -0.8705, -0.5199],\n",
      "        [ 0.9541, -0.9703, -0.5733],\n",
      "        [ 0.7270, -0.9185, -0.5882],\n",
      "        [ 1.1176, -1.0502, -0.2573]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  37\n",
      "Batch:  (tensor([[  101, 10329,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  2469,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  2017,  1005,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  4067,  2017,  ...,     0,     0,     0],\n",
      "        [  101,  2003,  2045,  ...,     0,     0,     0],\n",
      "        [  101,  2182,  2017,  ...,     0,     0,     0]]), tensor([0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.7979, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0789, -1.0448, -0.0172],\n",
      "        [ 1.0532, -1.1323, -0.2457],\n",
      "        [ 0.9080, -1.2656, -0.4381],\n",
      "        [ 1.0036, -1.1076, -0.3632],\n",
      "        [ 0.9541, -1.0447, -0.2647],\n",
      "        [ 1.1764, -1.0921, -0.1840],\n",
      "        [ 1.1321, -1.0420, -0.2646],\n",
      "        [ 0.9063, -1.2757, -0.2002],\n",
      "        [ 1.0296, -1.0825, -0.4252],\n",
      "        [ 0.8365, -1.0821, -0.1352],\n",
      "        [ 0.7513, -0.8731, -0.3181],\n",
      "        [ 1.0129, -1.0612, -0.4608],\n",
      "        [ 1.0601, -0.9486, -0.4378],\n",
      "        [ 1.0143, -0.9946, -0.0809],\n",
      "        [ 0.9492, -0.9904, -0.1927],\n",
      "        [ 0.9136, -1.2433, -0.0781],\n",
      "        [ 1.1175, -1.0082, -0.2148],\n",
      "        [ 1.2843, -1.1941, -0.0064],\n",
      "        [ 1.0176, -1.1611, -0.0936],\n",
      "        [ 1.0158, -1.0920, -0.5956],\n",
      "        [ 1.4060, -0.9203, -0.5503],\n",
      "        [ 0.9874, -1.1532,  0.0128],\n",
      "        [ 1.1133, -1.0440,  0.2508],\n",
      "        [ 0.9869, -1.1753, -0.4215],\n",
      "        [ 1.0327, -1.2253, -0.2436]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Batch  38\n",
      "Batch:  (tensor([[  101,  2469,  1012,  ...,     0,     0,     0],\n",
      "        [  101,  4067,  2017,  ...,     0,     0,     0],\n",
      "        [  101,  2279, 20587,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  7592,  1998,  ...,     0,     0,     0],\n",
      "        [  101,  2092,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  2745,  1045,  ...,     0,     0,     0]]), tensor([0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0,\n",
      "        0]))\n",
      "Outputs:  SequenceClassifierOutput(loss=tensor(0.6506, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7067, -1.1204, -0.0650],\n",
      "        [ 0.8120, -1.1877, -0.0612],\n",
      "        [ 1.1404, -0.8414, -0.3193],\n",
      "        [ 0.8440, -1.1263, -0.1772],\n",
      "        [ 0.8479, -1.2428, -0.1597],\n",
      "        [ 1.0878, -1.1937, -0.2127],\n",
      "        [ 0.8914, -1.1026, -0.0668],\n",
      "        [ 0.5669, -1.0195, -0.5009],\n",
      "        [ 0.9067, -1.0771, -0.2461],\n",
      "        [ 0.9456, -0.9794,  0.0432],\n",
      "        [ 0.9913, -1.1544, -0.1393],\n",
      "        [ 0.8786, -0.9208,  0.0123],\n",
      "        [ 1.1090, -1.2698, -0.1440],\n",
      "        [ 0.8110, -1.3337, -0.2822],\n",
      "        [ 0.7715, -0.9204, -0.2877],\n",
      "        [ 0.8799, -1.0454, -0.0818],\n",
      "        [ 1.0084, -1.1976,  0.0571],\n",
      "        [ 0.9702, -1.1496, -0.3709],\n",
      "        [ 0.9204, -0.8708, -0.0827],\n",
      "        [ 0.9488, -1.1543,  0.0227],\n",
      "        [ 0.9583, -0.9601,  0.0567],\n",
      "        [ 0.8487, -1.0506, -0.0435],\n",
      "        [ 0.8288, -1.1678,  0.1178],\n",
      "        [ 1.1083, -0.8262,  0.1419],\n",
      "        [ 0.9229, -1.1962,  0.1738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     61\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 63\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     65\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#All key inputs up here\n",
    "num_labels = 3  # Number of labels (right now it's neutral 0, bad 1, good 2)\n",
    "MAX_LENGTH = 512\n",
    "batch_size = 25 # Number for minibatch training here\n",
    "num_epochs = 100 # Number of training epochs\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the device we want to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Step 0 Done\")\n",
    "# Step 1: Preprocessing the data\n",
    "# Tokenize the text data\n",
    "tokenized_texts = []\n",
    "labels = []\n",
    "for i, row in temp.iterrows():\n",
    "    \n",
    "    tokenized_text = tokenizer.encode(row['Text'], add_special_tokens=True, max_length=512, truncation=True)\n",
    "    tokenized_texts.append(tokenized_text)\n",
    "    labels.append(row['Labels'])\n",
    "\n",
    "print(\"Step 1 Done\")\n",
    "# Define the label mapping\n",
    "label_map = {0: 0, -1: 1, 1: 2}\n",
    "\n",
    "# Change labels to be consistent with label mapping above\n",
    "labels = [label_map[label] for label in labels]\n",
    "\n",
    "# Step 2: Create dataloader\n",
    "\n",
    "input_ids = torch.tensor([tokenized_text[:MAX_LENGTH] + [0] * (MAX_LENGTH - len(tokenized_text[:MAX_LENGTH])) for tokenized_text in tokenized_texts])\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create dataloader\n",
    "data = TensorDataset(input_ids, labels)\n",
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "print(\"Step 2 Done\")\n",
    "# Step 3: Define the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "model.to(device)  # Move the model to the right device\n",
    "print(\"Step 3 Done\")\n",
    "# Step 4: Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "print(\"Step 4 Done\")\n",
    "# Step 5: Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    print(\"prebatch\")\n",
    "    count = 0\n",
    "    for batch in dataloader:\n",
    "        print(\"Batch \", count +1)\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, labels = batch\n",
    "        print(\"Batch: \", batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        print(\"Outputs: \", outputs)\n",
    "        loss = outputs.loss\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "\n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "# Step 6: Define a prediction function\n",
    "def predict(text):\n",
    "    # Tokenize the input text\n",
    "    tokenized_text = tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True)\n",
    "    \n",
    "    # Convert tokenized input to tensor and move it to the device\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Set the model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Apparently turning off grad saves memory and computation\n",
    "    with torch.no_grad():\n",
    "        # Give model the inputs\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        # Get the logits from the model's output\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate the probabilities using softmax\n",
    "        probabilities = torch.softmax(logits, dim=-1).squeeze(0)\n",
    "        \n",
    "        # Get the predicted label\n",
    "        predicted_label = torch.argmax(probabilities).item()\n",
    "        \n",
    "        # Return the predicted label and its probability\n",
    "        return predicted_label, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses = []\n",
    "\n",
    "company = t_100[tickers[50]]\n",
    "company_price_df = dict_of_df[tickers[50]]\n",
    "#Transform Date column so that the time of day is not included for price data (makes look up easier)\n",
    "company_price_df[\"Date\"] = company_price_df[\"Date\"].map(lambda x: x.split()[0])\n",
    "#Iterate through years\n",
    "for year in YEARS:\n",
    "    #Check to see if a earnings call has been reported for the given year\n",
    "    if len(company[year].keys()) != 0:\n",
    "        #Iterate through all the quarters that have a released earnings call\n",
    "        for quarter in company[year].keys():\n",
    "            dev_texts =[]\n",
    "            #Grabs date and transcript of the earnings call\n",
    "            date = company[year][quarter][\"date\"].split()\n",
    "            transcript = company[year][quarter][\"transcript\"]\n",
    "\n",
    "            #Checks to see when the call is released\n",
    "            #If the call is in the middle of the day we want the close price of the previous day\n",
    "            #If call is after hours we use the close price of that day\n",
    "            time = date[1].split(\":\")\n",
    "            date_0 = 0\n",
    "            if int(time[0] + time[1]) < 1600:\n",
    "                date_0 = (company_price_df[\"Date\"] == date[0]).shift(-1, fill_value = False)\n",
    "                #print(\"first\", company_price_df[\"Date\"], date[0])\n",
    "            else:\n",
    "                date_0 = company_price_df[\"Date\"] == date[0]\n",
    "                #print(\"second\", sum(date_0))\n",
    "            if sum(date_0) != 0:\n",
    "                #Grabs the Close price prior to earnings call, 20 days after, and 60 days after\n",
    "                date_20 = date_0.shift(20, fill_value = False)\n",
    "                date_60 = date_0.shift(60, fill_value = False)\n",
    "                data_on_dates = company_price_df[date_0 + date_20 + date_60]\n",
    "                close_price_0, close_price_20, close_price_60 = data_on_dates[\"Close\"]\n",
    "\n",
    "                #Calculates the one month and three month price change\n",
    "                one_month_change = ((close_price_20 - close_price_0) / close_price_0) * 100\n",
    "                three_month_change = ((close_price_60 - close_price_0) / close_price_0) * 100\n",
    "\n",
    "                #Creates a label for whether the stock has gone up, down, or stayed the same after the call release\n",
    "                label = 0\n",
    "                if one_month_change > 5 and three_month_change > 10:\n",
    "                    label = 1\n",
    "                elif one_month_change < -5 and three_month_change < -10:\n",
    "                    label = -1\n",
    "\n",
    "                sentences = transcript.split(\"\\n\")\n",
    "                for sentence in sentences:\n",
    "                    check = sentence.split()\n",
    "                    check1 = sentence.split(\":\")\n",
    "                    if len(check) !=0 and len(check) < 513:\n",
    "                        if check[0] not in bad_starts and len(check1) == 2:\n",
    "                            #print(sentence.split(\":\"))\n",
    "                            dev_texts.append(sentence.split(\":\")[1])\n",
    "                \n",
    "                first_method = []\n",
    "                second_method = []\n",
    "                for text in dev_texts:\n",
    "                    predicted_label, probability = predict(text)\n",
    "                    first_method.append(predicted_label)\n",
    "                    second_method.append(probability)\n",
    "                \n",
    "                first_method_pred = torch.argmax(torch.Tensor([first_method.count(0), first_method.count(1), first_method.count(2)])).item()\n",
    "                second_method_pred = torch.argmax(sum(second_method_pred)).item()\n",
    "\n",
    "                guesses.append((second_method_pred, label))\n",
    "                print(\"Method 1 Predition: \", first_method_pred)\n",
    "                print(\"Method 2 Predition: \", second_method_pred)\n",
    "                print(\"Actual Labe: \", (label + 3)%3)\n",
    "\n",
    "print(\"All Guesses: \", guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = [(1,2), (2,3), (3,4)]\n",
    "r[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
