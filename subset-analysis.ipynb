{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bh/dckst2q54p57m_p499z1mdz80000gn/T/ipykernel_21865/990176165.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "#The below code is still a work-in-progress! Need to actually train this model on downstream task, i.e., emotion recognition.\n",
    "\n",
    "# Load pre-trained BERT and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)  # 5 emotions\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the input text\n",
    "text = \"This is an example sentence.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Get the output probabilities\n",
    "output = model(**encoded_input)\n",
    "probabilities = output.logits.softmax(dim=-1)\n",
    "\n",
    "# Map the probabilities to numerical values\n",
    "anger_score = probabilities[0, 0].item()\n",
    "happiness_score = probabilities[0, 1].item()\n",
    "#and so on for other emotions. Not sure if want to stick with emotions or just go with positive/negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14843444526195526, 0.17139409482479095)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open file and read its contents\n",
    "import os\n",
    "file_path = os.path.join(\"Transcripts\", \"1Q23-27.4.23-AMZN-Q.txt\")\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    file_contents = file.read()\n",
    "\n",
    "# Now, file_contents variable contains the contents of your text file\n",
    "\n",
    "encoded_input = tokenizer(file_contents, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Get the output probabilities\n",
    "output = model(**encoded_input)\n",
    "probabilities = output.logits.softmax(dim=-1)\n",
    "\n",
    "# Map the probabilities to numerical values\n",
    "anger_score = probabilities[0, 0].item()\n",
    "happiness_score = probabilities[0, 1].item()\n",
    "anger_score, happiness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(max_iter=1000)\n",
    "\n",
    "#Below is extremely straightforward logistic regression code to train the model on whatever quantitative outputs BERT pumps out\n",
    "#LR.fit(X_train[cols], y_train)\n",
    "#print(LR.score(X_train[cols], y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
