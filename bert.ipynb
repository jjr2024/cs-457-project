{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains a prelim BERT fine tuning implementation with the Adam algo for optimization (basically fancy gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import requests\n",
    "import json \n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load an example csv with \"label\" and \"text\" columns. Then it fine-tunes BERT on that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data = pd.read_csv(\"test_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Eric Sheridan UBS Analyst Thanks for taking th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eric Sheridan Goldman Sachs Thank you very muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brian Nowak Morgan Stanley Great thanks for ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Doug Anmuth JP Morgan Thanks for taking the qu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "15  Eric Sheridan UBS Analyst Thanks for taking th...\n",
       "9   Eric Sheridan Goldman Sachs Thank you very muc...\n",
       "0   Brian Nowak Morgan Stanley Great thanks for ta...\n",
       "8   Doug Anmuth JP Morgan Thanks for taking the qu..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_json(\"prelim-data.json\")\n",
    "\n",
    "X = df_data.drop(columns=['label'])\n",
    "y = df_data['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "df_train = X_train.join(y_train)\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 corresponds to 0 (neutral). 1 corresponds to -1 (bad). 2 corresponds to 1 (good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\james\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Average Training Loss: 1.1795010964075725\n",
      "Epoch 2/30, Average Training Loss: 1.0549249450365703\n",
      "Epoch 3/30, Average Training Loss: 0.9830617904663086\n",
      "Epoch 4/30, Average Training Loss: 0.9564996163050333\n",
      "Epoch 5/30, Average Training Loss: 0.8210522532463074\n",
      "Epoch 6/30, Average Training Loss: 0.6882428328196207\n",
      "Epoch 7/30, Average Training Loss: 0.600182056427002\n",
      "Epoch 8/30, Average Training Loss: 0.44618914524714154\n",
      "Epoch 9/30, Average Training Loss: 0.35177040100097656\n",
      "Epoch 10/30, Average Training Loss: 0.27607104182243347\n",
      "Epoch 11/30, Average Training Loss: 0.21334673464298248\n",
      "Epoch 12/30, Average Training Loss: 0.16591536502043405\n",
      "Epoch 13/30, Average Training Loss: 0.14265652745962143\n",
      "Epoch 14/30, Average Training Loss: 0.10577250520388286\n",
      "Epoch 15/30, Average Training Loss: 0.0890015537540118\n",
      "Epoch 16/30, Average Training Loss: 0.06503093490997951\n",
      "Epoch 17/30, Average Training Loss: 0.05720130602518717\n",
      "Epoch 18/30, Average Training Loss: 0.04246301328142484\n",
      "Epoch 19/30, Average Training Loss: 0.03842635080218315\n",
      "Epoch 20/30, Average Training Loss: 0.033365264534950256\n",
      "Epoch 21/30, Average Training Loss: 0.028293367475271225\n",
      "Epoch 22/30, Average Training Loss: 0.024914974346756935\n",
      "Epoch 23/30, Average Training Loss: 0.021577797830104828\n",
      "Epoch 24/30, Average Training Loss: 0.02042028432091077\n",
      "Epoch 25/30, Average Training Loss: 0.017142864565054577\n",
      "Epoch 26/30, Average Training Loss: 0.01713912623624007\n",
      "Epoch 27/30, Average Training Loss: 0.016259461641311646\n",
      "Epoch 28/30, Average Training Loss: 0.014647151343524456\n",
      "Epoch 29/30, Average Training Loss: 0.013835549044112364\n",
      "Epoch 30/30, Average Training Loss: 0.013604353492458662\n"
     ]
    }
   ],
   "source": [
    "#All key inputs up here\n",
    "num_labels = 3  # Number of labels (right now it's neutral 0, bad 1, good 2)\n",
    "MAX_LENGTH = 128\n",
    "batch_size = 7  # Number for minibatch training here\n",
    "num_epochs = 30 # Number of training epochs\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the device we want to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 1: Preprocessing the data\n",
    "# Tokenize the text data\n",
    "tokenized_texts = []\n",
    "labels = []\n",
    "for i, row in df_train.iterrows():\n",
    "    tokenized_text = tokenizer.encode(row['text'], add_special_tokens=True, max_length=512, truncation=True)\n",
    "    tokenized_texts.append(tokenized_text)\n",
    "    labels.append(row['label'])\n",
    "\n",
    "# Define the label mapping\n",
    "label_map = {0: 0, -1: 1, 1: 2}\n",
    "\n",
    "# Change labels to be consistent with label mapping above\n",
    "labels = [label_map[label] for label in labels]\n",
    "\n",
    "# Step 2: Create dataloader\n",
    "input_ids = torch.tensor([tokenized_text[:MAX_LENGTH] + [0] * (MAX_LENGTH - len(tokenized_text[:MAX_LENGTH])) for tokenized_text in tokenized_texts])\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create dataloader\n",
    "data = TensorDataset(input_ids, labels)\n",
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Step 3: Define the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "model.to(device)  # Move the model to the right device\n",
    "\n",
    "# Step 4: Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=0.00003)\n",
    "\n",
    "# Step 5: Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "# Step 6: Define a prediction function\n",
    "def predict(text):\n",
    "    # Tokenize the input text\n",
    "    tokenized_text = tokenizer.encode(text, add_special_tokens=True, max_length = 512, truncation=True)\n",
    "    \n",
    "    # Convert tokenized input to tensor and move it to the device\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Set the model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Apparently turning off grad saves memory and computation\n",
    "    with torch.no_grad():\n",
    "        # Give model the inputs\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        # Get the logits from the model's output\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate the probabilities using softmax\n",
    "        probabilities = torch.softmax(logits, dim=-1).squeeze(0)\n",
    "        \n",
    "        # Get the predicted label\n",
    "        predicted_label = torch.argmax(probabilities).item()\n",
    "        \n",
    "        # Return the predicted label and probabilities\n",
    "        return probabilities, predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model is your trained model\n",
    "# Initialize a list to store the predictions\n",
    "predictions_label0 = []\n",
    "predictions_label1 = []\n",
    "predictions_label2 = []\n",
    "predictions_predict = []\n",
    "# Iterate over each observation in X_test\n",
    "\n",
    "def getBERTScores(df):\n",
    "    predictions_label0 = []\n",
    "    predictions_label1 = []\n",
    "    predictions_label2 = []\n",
    "    predictions_predict = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        predictions = predict(row[\"text\"])\n",
    "        \n",
    "        # Append the prediction to the list\n",
    "        #print(predictions)\n",
    "        #print(predictions[0])\n",
    "        predictions_label0.append(predictions[0][0])\n",
    "        predictions_label1.append(predictions[0][1])\n",
    "        predictions_label2.append(predictions[0][2])\n",
    "        predictions_predict.append(predictions[1])\n",
    "\n",
    "    # Add the predictions as a new feature to X_test\n",
    "    rev_label_map = {0: 0, 1: -1, 2: 1}\n",
    "    predictions_predict = [rev_label_map[label] for label in predictions_predict]\n",
    "\n",
    "    df['neutral'] = predictions_label0\n",
    "    df['bad'] = predictions_label1\n",
    "    df['good'] = predictions_label2\n",
    "    df[\"predict\"] = predictions_predict\n",
    "\n",
    "\n",
    "for index, row in X_test.iterrows():\n",
    "    predictions = predict(row[\"text\"])\n",
    "    \n",
    "    # Append the prediction to the list\n",
    "    #print(predictions)\n",
    "    #print(predictions[0])\n",
    "    predictions_label0.append(predictions[0][0])\n",
    "    predictions_label1.append(predictions[0][1])\n",
    "    predictions_label2.append(predictions[0][2])\n",
    "    predictions_predict.append(predictions[1])\n",
    "\n",
    "# Add the predictions as a new feature to X_test\n",
    "rev_label_map = {0: 0, 1: -1, 2: 1}\n",
    "predictions_predict = [rev_label_map[label] for label in predictions_predict]\n",
    "\n",
    "X_test['neutral'] = predictions_label0\n",
    "X_test['bad'] = predictions_label1\n",
    "X_test['good'] = predictions_label2\n",
    "X_test[\"predict\"] = predictions_predict\n",
    "\n",
    "\n",
    "# Now X_test contains the original features along with the predicted labels as a new feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text         neutral  \\\n",
      "15  Eric Sheridan UBS Analyst Thanks for taking th...  tensor(0.0759)   \n",
      "9   Eric Sheridan Goldman Sachs Thank you very muc...  tensor(0.0499)   \n",
      "0   Brian Nowak Morgan Stanley Great thanks for ta...  tensor(0.0840)   \n",
      "8   Doug Anmuth JP Morgan Thanks for taking the qu...  tensor(0.0382)   \n",
      "\n",
      "               bad            good  predict  \n",
      "15  tensor(0.5981)  tensor(0.3260)       -1  \n",
      "9   tensor(0.2932)  tensor(0.6569)        1  \n",
      "0   tensor(0.7306)  tensor(0.1854)       -1  \n",
      "8   tensor(0.1091)  tensor(0.8527)        1  \n",
      "15    1\n",
      "9     1\n",
      "0    -1\n",
      "8    -1\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "(X_test[\"predict\"] == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7368421052631579"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getBERTScores(X_train)\n",
    "\n",
    "(X_train[\"predict\"] == y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7894736842105263\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "getBERTScores(X_train)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(max_iter=100)\n",
    "LR.fit(X_train[[\"neutral\", \"good\", \"bad\"]], y_train)\n",
    "print(LR.score(X_train[[\"neutral\", \"good\", \"bad\"]], y_train))\n",
    "getBERTScores(X_test)\n",
    "print(LR.score(X_test[[\"neutral\", \"good\", \"bad\"]], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.526316\n",
      "Name: label, dtype: float64\n",
      "1    0.5\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "most_frequent_items = y_train.value_counts()\n",
    "most_frequent = most_frequent_items.head(1)\n",
    "print(most_frequent/len(y_train))\n",
    "most_frequent_items = y_test.value_counts()\n",
    "most_frequent = most_frequent_items.head(1)\n",
    "print(most_frequent/len(y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
