{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertModel\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a csv with \"label\" and \"text\" columns. Then it fine-tunes BERT on that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"GOOG_transcripts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 363)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns = [\"text\", \"label\"]\n",
    "df_data = df_data.dropna()\n",
    "X = df_data.drop(columns=['label'])\n",
    "y = df_data['label']\n",
    "\n",
    "# Split the data into train, dev, and test sets (80-10-10 split)\n",
    "X_train, X_devtest, y_train, y_devtest = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_devtest, y_devtest, test_size=0.5)\n",
    "\n",
    "df_train = X_train.join(y_train)\n",
    "df_dev = X_dev.join(y_dev)\n",
    "df_test = X_test.join(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the created sub-datasets\n",
    "df_train.to_csv('GOOG_final_train_set.csv', index=False)\n",
    "df_dev.to_csv('GOOG_final_dev_set.csv', index=False)\n",
    "df_test.to_csv('GOOG_final_test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 corresponds to 0 (neutral). 1 corresponds to -1 (bad). 2 corresponds to 1 (good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#All key inputs up here\n",
    "num_labels = 3  # Number of labels (neutral 0, bad 1, good 2)\n",
    "MAX_LENGTH = 128\n",
    "batch_size = 5  # Number for minibatch training here\n",
    "num_epochs = 20 # Number of training epochs\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the device we want to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "model.to(device)  # Move the model to the right device\n",
    "\n",
    "def data_preprocess(df):\n",
    "    # Preprocessing the data\n",
    "    # Tokenize the text data\n",
    "    tokenized_texts = []\n",
    "    labels = []\n",
    "    for i, row in df.iterrows():\n",
    "        tokenized_text = tokenizer.encode(row['text'], add_special_tokens=True, max_length=512, truncation=True)\n",
    "        tokenized_texts.append(tokenized_text)\n",
    "        labels.append(row['label'])\n",
    "\n",
    "    # Define the label mapping\n",
    "    label_map = {0: 0, -1: 1, 1: 2}\n",
    "\n",
    "    # Change labels to be consistent with label mapping above\n",
    "    labels = [label_map[label] for label in labels]\n",
    "\n",
    "    input_ids = torch.tensor([tokenized_text[:MAX_LENGTH] + [0] * (MAX_LENGTH - len(tokenized_text[:MAX_LENGTH])) for tokenized_text in tokenized_texts])\n",
    "    labels = torch.tensor(labels)\n",
    "    # Output data that's ready to be put into a dataloader\n",
    "    data = TensorDataset(input_ids, labels)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "loss_vec = []\n",
    "def train_model(lr, batch_size, num_epochs, data):\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr) # Define the optimizer\n",
    "    avg_train_loss = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, labels = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_train_loss}\")\n",
    "    loss_vec.append(avg_train_loss)\n",
    "\n",
    "# Prediction function\n",
    "def predict(text):\n",
    "    # Tokenize text\n",
    "    tokenized_text = tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True)\n",
    "    \n",
    "    # Convert tokenized input to tensor and move it to the device\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Set the model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Give model the inputs\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        # Get the logits from the model's output\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Calculate the probabilities using softmax\n",
    "    probabilities = torch.softmax(logits, dim=-1).squeeze(0)\n",
    "    \n",
    "    # Get the predicted label\n",
    "    predicted_label = torch.argmax(probabilities).item()\n",
    "    \n",
    "    # Return the predicted label and probabilities\n",
    "    return probabilities, predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 1e-05\n",
      "Batch Size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Training Loss: 1.0973673877508745\n",
      "Epoch 2/3, Average Training Loss: 1.0841116671976836\n",
      "Epoch 3/3, Average Training Loss: 1.0827311510625093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32\n",
      "Epoch 1/3, Average Training Loss: 1.0848345359166462\n",
      "Epoch 2/3, Average Training Loss: 1.068823218345642\n",
      "Epoch 3/3, Average Training Loss: 1.052112028002739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 48\n",
      "Epoch 1/3, Average Training Loss: 1.0955205708742142\n",
      "Epoch 2/3, Average Training Loss: 1.0713668167591095\n",
      "Epoch 3/3, Average Training Loss: 1.0669596642255783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 2e-05\n",
      "Batch Size: 16\n",
      "Epoch 1/3, Average Training Loss: 1.0927387527797534\n",
      "Epoch 2/3, Average Training Loss: 1.080745132073112\n",
      "Epoch 3/3, Average Training Loss: 1.0476503735003264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32\n",
      "Epoch 1/3, Average Training Loss: 1.1033319632212322\n",
      "Epoch 2/3, Average Training Loss: 1.0801902115345001\n",
      "Epoch 3/3, Average Training Loss: 1.089866171280543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 48\n",
      "Epoch 1/3, Average Training Loss: 1.1004212945699692\n",
      "Epoch 2/3, Average Training Loss: 1.0750447809696198\n",
      "Epoch 3/3, Average Training Loss: 1.0720806121826172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 3.0000000000000004e-05\n",
      "Batch Size: 16\n",
      "Epoch 1/3, Average Training Loss: 1.1241115798120913\n",
      "Epoch 2/3, Average Training Loss: 1.0773768450902856\n",
      "Epoch 3/3, Average Training Loss: 1.0681819760281106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32\n",
      "Epoch 1/3, Average Training Loss: 1.0841793020566304\n",
      "Epoch 2/3, Average Training Loss: 1.0966764986515045\n",
      "Epoch 3/3, Average Training Loss: 1.0867029031117756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 48\n",
      "Epoch 1/3, Average Training Loss: 1.0827585011720657\n",
      "Epoch 2/3, Average Training Loss: 1.0672740638256073\n",
      "Epoch 3/3, Average Training Loss: 1.0567666888237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 4e-05\n",
      "Batch Size: 16\n",
      "Epoch 1/3, Average Training Loss: 1.1033117745233618\n",
      "Epoch 2/3, Average Training Loss: 1.0838865845099739\n",
      "Epoch 3/3, Average Training Loss: 1.082493932350822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32\n",
      "Epoch 1/3, Average Training Loss: 1.098680466413498\n",
      "Epoch 2/3, Average Training Loss: 1.1126365264256795\n",
      "Epoch 3/3, Average Training Loss: 1.0829573074976604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 48\n",
      "Epoch 1/3, Average Training Loss: 1.1407432854175568\n",
      "Epoch 2/3, Average Training Loss: 1.0871445536613464\n",
      "Epoch 3/3, Average Training Loss: 1.0827119946479797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 5e-05\n",
      "Batch Size: 16\n",
      "Epoch 1/3, Average Training Loss: 1.0859093769736912\n",
      "Epoch 2/3, Average Training Loss: 1.0805319625398386\n",
      "Epoch 3/3, Average Training Loss: 1.05078619459401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32\n",
      "Epoch 1/3, Average Training Loss: 1.119448572397232\n",
      "Epoch 2/3, Average Training Loss: 1.097564309835434\n",
      "Epoch 3/3, Average Training Loss: 1.078416183590889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 48\n",
      "Epoch 1/3, Average Training Loss: 1.1220953315496445\n",
      "Epoch 2/3, Average Training Loss: 1.0897968709468842\n",
      "Epoch 3/3, Average Training Loss: 1.0725366324186325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Below is our dev loop: finding the best hyperparameters to use for the training loop\n",
    "data = data_preprocess(df_dev)\n",
    "lr_vec = []\n",
    "batch_size_vec = []\n",
    "for i in range(1,6):\n",
    "    print(f\"Learning Rate: {i*1e-5}\")\n",
    "    for j in range (1,4):\n",
    "        print(f\"Batch Size: {j*16}\")\n",
    "        lr_vec.append(i*1e-5)\n",
    "        batch_size_vec.append(j*16)\n",
    "        train_model(lr=i*1e-5, batch_size=j*16, num_epochs=3, data=data)\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find the minimum final epoch loss. Then we set best_lr and best_bsize to the learning rate and batch sizes that correspond to that minimum loss\n",
    "best_lr = lr_vec[loss_vec.index(min(loss_vec))]\n",
    "best_bsize = batch_size_vec[loss_vec.index(min(loss_vec))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest loss, best learning rate, batch size (1.0476503735003264, 2e-05, 16)\n",
      "Epoch 1/10, Average Training Loss: 1.0755329941000258\n",
      "Epoch 2/10, Average Training Loss: 1.0543964125297882\n",
      "Epoch 3/10, Average Training Loss: 1.0258540717455058\n",
      "Epoch 4/10, Average Training Loss: 0.9773261514338818\n",
      "Epoch 5/10, Average Training Loss: 0.8816695282092462\n",
      "Epoch 6/10, Average Training Loss: 0.7084242532868962\n",
      "Epoch 7/10, Average Training Loss: 0.5140244630830628\n",
      "Epoch 8/10, Average Training Loss: 0.36389172916392704\n",
      "Epoch 9/10, Average Training Loss: 0.23798746844896904\n",
      "Epoch 10/10, Average Training Loss: 0.21808725665067577\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "print(f\"Lowest loss, best learning rate, batch size {min(loss_vec), best_lr, best_bsize}\")\n",
    "data = data_preprocess(df_train)\n",
    "train_model(lr=best_lr, batch_size=best_bsize, num_epochs=10, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert_train.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional code to save training model\n",
    "dump(model, 'bert_train.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commented out: code to load an already trained model\n",
    "#model = load('bert_train.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the predictions\n",
    "predictions_label0 = []\n",
    "predictions_label1 = []\n",
    "predictions_label2 = []\n",
    "predictions_predict = []\n",
    "# Iterate over each observation in input df\n",
    "def getBERTScores(df):\n",
    "    predictions_label0 = []\n",
    "    predictions_label1 = []\n",
    "    predictions_label2 = []\n",
    "    predictions_predict = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        predictions = predict(row[\"text\"])\n",
    "        \n",
    "        # Append the prediction to the list\n",
    "        predictions_label0.append(predictions[0][0])\n",
    "        predictions_label1.append(predictions[0][1])\n",
    "        predictions_label2.append(predictions[0][2])\n",
    "        predictions_predict.append(predictions[1])\n",
    "\n",
    "    # Add the predictions as a new feature to X_test\n",
    "    rev_label_map = {0: 0, 1: -1, 2: 1}\n",
    "    predictions_predict = [rev_label_map[label] for label in predictions_predict]\n",
    "\n",
    "    df['neutral'] = predictions_label0\n",
    "    df['bad'] = predictions_label1\n",
    "    df['good'] = predictions_label2\n",
    "    df[\"predict\"] = predictions_predict\n",
    "\n",
    "getBERTScores(X_test)\n",
    "# Now X_test contains the original features along with the predicted labels as a new feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4793388429752066"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find our overall accuracy\n",
    "(X_test[\"predict\"] == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7144827586206897"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getBERTScores(X_train)\n",
    "getBERTScores(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7144827586206897\n",
      "Testing Accuracy: 0.4793388429752066\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {(X_train['predict'] == y_train).mean()}\")\n",
    "print(f\"Testing Accuracy: {(X_test['predict'] == y_test).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark for Train Set 0    0.433448\n",
      "Name: label, dtype: float64\n",
      "Benchmark for Test Set 1    0.399449\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "most_frequent_items = y_train.value_counts()\n",
    "most_frequent = most_frequent_items.head(1)\n",
    "print(f\"Benchmark for Train Set {most_frequent/len(y_train)}\")\n",
    "most_frequent_items = y_test.value_counts()\n",
    "most_frequent = most_frequent_items.head(1)\n",
    "print(f\"Benchmark for Test Set {most_frequent/len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of test data labeled 0: 0.39118457300275483\n",
      "Proportion of test data labeled 1: 0.39944903581267216\n",
      "Proportion of test data labeled -1: 0.209366391184573\n",
      "Number of test observations: 363\n",
      "Proportion of training data labeled 0: 0.43344827586206897\n",
      "Proportion of training data labeled 1: 0.3510344827586207\n",
      "Proportion of training data labeled -1: 0.21551724137931033\n",
      "Number of training observations: 2900\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of test data labeled 0:\", y_test[y_test == 0].shape[0] / y_test.shape[0])\n",
    "print(\"Proportion of test data labeled 1:\", y_test[y_test == 1].shape[0] / y_test.shape[0])\n",
    "print(\"Proportion of test data labeled -1:\", y_test[y_test == -1].shape[0] / y_test.shape[0])\n",
    "print(f\"Number of test observations: {y_test.shape[0]}\")\n",
    "\n",
    "print(\"Proportion of training data labeled 0:\", y_train[y_train == 0].shape[0] / y_train.shape[0])\n",
    "print(\"Proportion of training data labeled 1:\", y_train[y_train == 1].shape[0] / y_train.shape[0])\n",
    "print(\"Proportion of training data labeled -1:\", y_train[y_train == -1].shape[0] / y_train.shape[0])\n",
    "print(f\"Number of training observations: {y_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of test data labeled 0: 0.8484848484848485\n",
      "Proportion of test data labeled 1: 0.11570247933884298\n",
      "Proportion of test data labeled -1: 0.03581267217630854\n",
      "Number of test observations: 363\n",
      "Proportion of training data labeled 0: 0.6913793103448276\n",
      "Proportion of training data labeled 1: 0.21551724137931033\n",
      "Proportion of training data labeled -1: 0.09310344827586207\n",
      "Number of training observations: 2900\n"
     ]
    }
   ],
   "source": [
    "train_predict = X_train['predict']\n",
    "test_predict = X_test['predict']\n",
    "\n",
    "print(\"Proportion of test data labeled 0:\", test_predict[test_predict == 0].shape[0] / test_predict.shape[0])\n",
    "print(\"Proportion of test data labeled 1:\", test_predict[test_predict == 1].shape[0] / test_predict.shape[0])\n",
    "print(\"Proportion of test data labeled -1:\", test_predict[test_predict == -1].shape[0] / test_predict.shape[0])\n",
    "print(f\"Number of test observations: {test_predict.shape[0]}\")\n",
    "\n",
    "print(\"Proportion of training data labeled 0:\", train_predict[train_predict == 0].shape[0] / train_predict.shape[0])\n",
    "print(\"Proportion of training data labeled 1:\", train_predict[train_predict == 1].shape[0] / train_predict.shape[0])\n",
    "print(\"Proportion of training data labeled -1:\", train_predict[train_predict == -1].shape[0] / train_predict.shape[0])\n",
    "print(f\"Number of training observations: {train_predict.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AAPL = pd.read_csv(\"AAPL_transcripts.csv\")\n",
    "df_AAPL.columns = [\"text\", \"label\"]\n",
    "df_AAPL = df_AAPL.dropna()\n",
    "\n",
    "#df_AAPL_sample = df_AAPL #df_AAPL.sample(frac=0.3)\n",
    "#df_AAPL_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "getBERTScores(df_AAPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of AAPL data labeled 0: 0.8379863706309079\n",
      "Proportion of AAPL data labeled 1: 0.12398329303143547\n",
      "Proportion of AAPL data labeled -1: 0.03803033633765663\n",
      "Number of AAPL observations: 4549\n",
      "AAPL Accuracy: 0.43635963948120465\n",
      "Benchmark for Test Set 0    0.43636\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "AAPL_predict = df_AAPL['predict']\n",
    "\n",
    "print(\"Proportion of AAPL data labeled 0:\", AAPL_predict[AAPL_predict == 0].shape[0] / AAPL_predict.shape[0])\n",
    "print(\"Proportion of AAPL data labeled 1:\", AAPL_predict[AAPL_predict == 1].shape[0] / AAPL_predict.shape[0])\n",
    "print(\"Proportion of AAPL data labeled -1:\", AAPL_predict[AAPL_predict == -1].shape[0] / AAPL_predict.shape[0])\n",
    "print(f\"Number of AAPL observations: {AAPL_predict.shape[0]}\")\n",
    "\n",
    "print(f\"AAPL Accuracy: {(df_AAPL['predict'] == df_AAPL['label']).mean()}\")\n",
    "\n",
    "most_frequent_items = df_AAPL['label'].value_counts()\n",
    "most_frequent = most_frequent_items.head(1)\n",
    "print(f\"Benchmark for Test Set {most_frequent/len(df_AAPL['label'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MSFT = pd.read_csv(\"MSFT_transcripts.csv\")\n",
    "df_MSFT.columns = [\"text\", \"label\"]\n",
    "df_MSFT = df_MSFT.dropna()\n",
    "\n",
    "#df_AAPL_sample = df_AAPL #df_AAPL.sample(frac=0.3)\n",
    "#df_AAPL_sample.shape[0]\n",
    "getBERTScores(df_MSFT)\n",
    "\n",
    "MSFT_predict = df_MSFT['predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of MSFT data with predicted label 0: 0.8280318091451292\n",
      "Proportion of MSFT data with predicted label 1: 0.13286944996686548\n",
      "Proportion of MSFT data with predicted label -1: 0.0390987408880053\n",
      "Number of MSFT observations: 3018\n",
      "MSFT Accuracy: 0.4188204108681246\n",
      "Benchmark for Test Set 0    0.429755\n",
      "Name: label, dtype: float64\n",
      "Proportion of MSFT data actually labeled 0: 0.42975480450629555\n",
      "Proportion of MSFT data actually labeled 1: 0.40059642147117297\n",
      "Proportion of MSFT data actually labeled -1: 0.16964877402253148\n",
      "3018\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of MSFT data with predicted label 0:\", MSFT_predict[MSFT_predict == 0].shape[0] / MSFT_predict.shape[0])\n",
    "print(\"Proportion of MSFT data with predicted label 1:\", MSFT_predict[MSFT_predict == 1].shape[0] / MSFT_predict.shape[0])\n",
    "print(\"Proportion of MSFT data with predicted label -1:\", MSFT_predict[MSFT_predict == -1].shape[0] / MSFT_predict.shape[0])\n",
    "print(f\"Number of MSFT observations: {MSFT_predict.shape[0]}\")\n",
    "\n",
    "print(f\"MSFT Accuracy: {(df_MSFT['predict'] == df_MSFT['label']).mean()}\")\n",
    "\n",
    "most_frequent_items = df_MSFT['label'].value_counts()\n",
    "most_frequent = most_frequent_items.head(1)\n",
    "print(f\"Benchmark for Test Set {most_frequent/len(df_MSFT['label'])}\")\n",
    "\n",
    "print(\"Proportion of MSFT data actually labeled 0:\", df_MSFT[df_MSFT[\"label\"] == 0].shape[0] / df_MSFT.shape[0])\n",
    "print(\"Proportion of MSFT data actually labeled 1:\", df_MSFT[df_MSFT[\"label\"] == 1].shape[0] / df_MSFT.shape[0])\n",
    "print(\"Proportion of MSFT data actually labeled -1:\", df_MSFT[df_MSFT[\"label\"] == -1].shape[0] / df_MSFT.shape[0])\n",
    "print(df_MSFT.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
